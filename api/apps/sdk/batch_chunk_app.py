#
#  Copyright 2024 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

"""
KnowFlow æ‰¹é‡ Chunk æ·»åŠ æ’ä»¶ (é›†æˆå¼å®ç°)
æä¾› POST /datasets/{dataset_id}/documents/{document_id}/chunks/batch æ¥å£
æä¾› GET /datasets/{dataset_id}/documents/{document_id}/parse/progress æ¥å£
æ‰€æœ‰ä¸šåŠ¡é€»è¾‘ç›´æ¥åœ¨æ­¤æ–‡ä»¶ä¸­å®ç°ï¼Œç®€åŒ–ç»“æ„
"""

import datetime
import xxhash
import re
import sys
import traceback
from timeit import default_timer as timer
import time
import threading

from flask import request, Blueprint
from api.utils.api_utils import token_required, get_result, get_error_data_result
from api.db.services.knowledgebase_service import KnowledgebaseService
from api.db.services.document_service import DocumentService
from api.db import LLMType, ParserType
from api.db.services.llm_service import TenantLLMService
from rag.nlp import rag_tokenizer, search
from rag.app.qa import rmPrefix, beAdoc
from rag.prompts import keyword_extraction, question_proposal
from rag.app.tag import label_question
from rag.utils import rmSpace
from graphrag.utils import get_llm_cache, set_llm_cache, chat_limiter
from api import settings
import trio

# åˆ›å»º Blueprint manager
manager = Blueprint('batch_chunk', __name__)

# å…¨å±€è¿›åº¦çŠ¶æ€å­˜å‚¨ - ç®€åŒ–ç‰ˆæœ¬
# ç»“æ„: {document_id: {"progress": float, "message": str, "timestamp": float, "stage": str}}
_progress_states = {}
_progress_lock = threading.Lock()


def _update_progress_state(document_id, progress=None, message=None, stage=None):
    """æ›´æ–°æ–‡æ¡£çš„è¿›åº¦çŠ¶æ€"""
    with _progress_lock:
        if document_id not in _progress_states:
            _progress_states[document_id] = {
                "progress": 0.0,
                "message": "",
                "timestamp": time.time(),
                "stage": "initializing"
            }
        
        state = _progress_states[document_id]
        if progress is not None:
            state["progress"] = float(progress)
        if message is not None:
            state["message"] = str(message)
        if stage is not None:
            state["stage"] = str(stage)
        state["timestamp"] = time.time()


def _get_progress_state(document_id):
    """è·å–æ–‡æ¡£çš„è¿›åº¦çŠ¶æ€"""
    with _progress_lock:
        if document_id in _progress_states:
            return _progress_states[document_id].copy()
        else:
            return {
                "progress": 0.0,
                "message": "æœªå¼€å§‹",
                "timestamp": time.time(),
                "stage": "unknown"
            }


def _clear_progress_state(document_id):
    """æ¸…ç©ºæŒ‡å®šæ–‡æ¡£çš„è¿›åº¦çŠ¶æ€"""
    with _progress_lock:
        if document_id in _progress_states:
            del _progress_states[document_id]
            print(f"ğŸ§¹ æ¸…ç©ºè¿›åº¦çŠ¶æ€: {document_id}")


def _process_single_batch(batch_chunks, batch_index, embd_mdl, doc, dataset_id, document_id, 
                         current_time, current_timestamp, tenant_id, DB_BULK_SIZE):
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„chunks
    
    Args:
        batch_chunks: å½“å‰æ‰¹æ¬¡çš„chunksæ•°æ® [(original_index, chunk_req), ...]
        batch_index: æ‰¹æ¬¡èµ·å§‹ç´¢å¼•
        embd_mdl: embeddingæ¨¡å‹å®ä¾‹
        doc: æ–‡æ¡£å¯¹è±¡
        dataset_id: æ•°æ®é›†ID
        document_id: æ–‡æ¡£ID
        current_time: å½“å‰æ—¶é—´å­—ç¬¦ä¸²
        current_timestamp: å½“å‰æ—¶é—´æˆ³
        tenant_id: ç§Ÿæˆ·ID
        DB_BULK_SIZE: æ•°æ®åº“æ‰¹é‡æ’å…¥å¤§å°
        
    Returns:
        tuple: (success: bool, result: dict)
               success=Trueæ—¶ï¼ŒresultåŒ…å« {"processed_chunks": [], "cost": float}
               success=Falseæ—¶ï¼ŒresultåŒ…å« {"error": str}
    """
    try:
        # æ„å»ºchunkæ–‡æ¡£æ•°æ®
        processed_chunks = []
        embedding_texts = []
        batch_cost = 0
        
        # ç®€åŒ–æ’åºæœºåˆ¶ï¼šå›ºå®špage_num_int=1ï¼Œç”¨top_intä¿è¯é¡ºåº
        for original_index, chunk_req in batch_chunks:
            content = chunk_req["content"]
            
            # ä½¿ç”¨ä» ragflow_build ä¼ é€’è¿‡æ¥çš„ top_intï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ original_index
            global_top_int = chunk_req.get("top_int", original_index)
            
            chunk_id = xxhash.xxh64((content + document_id + str(global_top_int)).encode("utf-8")).hexdigest()
            
            # åŸºç¡€chunkæ•°æ®ç»“æ„
            d = {
                "id": chunk_id,
                "content_ltks": rag_tokenizer.tokenize(content),
                "content_with_weight": content,
                "content_sm_ltks": rag_tokenizer.fine_grained_tokenize(rag_tokenizer.tokenize(content)),
                "important_kwd": chunk_req.get("important_keywords", []),
                "important_tks": rag_tokenizer.tokenize(" ".join(chunk_req.get("important_keywords", []))),
                "question_kwd": [str(q).strip() for q in chunk_req.get("questions", []) if str(q).strip()],
                "question_tks": rag_tokenizer.tokenize("\n".join(chunk_req.get("questions", []))),
                "create_time": current_time,
                "create_timestamp_flt": current_timestamp,
                "kb_id": dataset_id,
                "docnm_kwd": doc.name,
                "doc_id": document_id,
                # ç»Ÿä¸€æ’åºæœºåˆ¶ï¼šå›ºå®špage_num_int=1ï¼Œtop_int=åŸå§‹ç´¢å¼•
                "page_num_int": [1],  # å›ºå®šä¸º1ï¼Œä¿è¯æ‰€æœ‰chunkséƒ½åœ¨åŒä¸€"é¡µ"
                "top_int": global_top_int  # ä½¿ç”¨å…¨å±€ç´¢å¼•ä¿è¯é¡ºåº
            }
            
            # ä½ç½®ä¿¡æ¯å¤„ç†ï¼ˆä½œä¸ºé¢å¤–ä¿¡æ¯ï¼Œä¸å½±å“æ’åºï¼‰
            if "positions" in chunk_req:
                # æ·»åŠ ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼Œä½†ä¸è¦†ç›–page_num_intå’Œtop_int
                _add_positions_to_chunk_data(d, chunk_req["positions"])
                print(f"[_process_single_batch] global_idx={global_top_int}: ç²¾ç¡®åæ ‡ + ç´¢å¼•æ’åº (page={d['page_num_int'][0]}, top={global_top_int})")
            
            # å‡†å¤‡embeddingæ–‡æœ¬
            text_for_embedding = content if not d["question_kwd"] else "\n".join(d["question_kwd"])
            embedding_texts.append([doc.name, text_for_embedding])
            processed_chunks.append(d)
            
            print(f"[_process_single_batch] chunk_idx={original_index}, content_len={len(chunk_req['content'])}, has_positions={'positions' in chunk_req}, top_int={d.get('top_int')}")
        
        # æ‰¹é‡æ‰§è¡Œembedding
        all_texts_for_embedding = []
        for doc_name, content_text in embedding_texts:
            all_texts_for_embedding.extend([doc_name, content_text])
        
        batch_vectors, batch_cost = embd_mdl.encode(all_texts_for_embedding)
        
        # æ·»åŠ å‘é‡åˆ°chunks
        for i, d in enumerate(processed_chunks):
            doc_name_vector = batch_vectors[i * 2]
            content_vector = batch_vectors[i * 2 + 1]
            v = 0.1 * doc_name_vector + 0.9 * content_vector
            d["q_%d_vec" % len(v)] = v.tolist()
        
        # åˆ†æ‰¹æ’å…¥æ•°æ®åº“
        for b in range(0, len(processed_chunks), DB_BULK_SIZE):
            batch_for_db = processed_chunks[b:b + DB_BULK_SIZE]
            try:
                settings.docStoreConn.insert(batch_for_db, search.index_name(tenant_id), dataset_id)
            except Exception as db_error:
                print(f"[_process_single_batch] DBå†™å…¥å¼‚å¸¸: {db_error}\n{traceback.format_exc()}")
                return False, {"error": f"Database insertion failed: {str(db_error)}"}
        
        return True, {"processed_chunks": processed_chunks, "cost": batch_cost}
        
    except Exception as e:
        print(f"[_process_single_batch] Batchå¤„ç†å¼‚å¸¸: {e}\n{traceback.format_exc()}")
        return False, {"error": str(e)}


def _process_auto_keywords_questions(all_processed_chunks, document_id, chat_model, 
                                    auto_keywords, auto_questions, tenant_id, dataset_id):
    """
    å¤„ç†è‡ªåŠ¨å…³é”®è¯å’Œé—®é¢˜ç”Ÿæˆ
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸå®Œæˆå¤„ç†
    """
    try:
        # åˆ›å»ºå¼‚æ­¥å¤„ç†å‡½æ•°
        async def process_batch_keywords_and_questions():
            # å…³é”®è¯æå–å¤„ç†
            keywords_processed = 0
            
            if auto_keywords > 0:
                st = timer()
                
                async def doc_keyword_extraction(chat_mdl, d, topn):
                    nonlocal keywords_processed
                    try:
                        content = d.get('content_with_weight', '').strip()
                        if not content or len(content) < 10:  # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
                            return
                        
                        # æ£€æŸ¥ç¼“å­˜
                        cached = get_llm_cache(chat_mdl.llm_name, content, "keywords", {"topn": topn})
                        if not cached:
                            async with chat_limiter:
                                # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
                                cached = await trio.to_thread.run_sync(
                                    lambda: keyword_extraction(chat_mdl, content, topn)
                                )
                            # åªç¼“å­˜æœ‰æ•ˆç»“æœ
                            if cached and cached.strip():
                                set_llm_cache(chat_mdl.llm_name, content, cached, "keywords", {"topn": topn})
                        
                        if cached and cached.strip():
                            d["important_kwd"] = cached.split(",")
                            d["important_tks"] = rag_tokenizer.tokenize(" ".join(d["important_kwd"]))
                            keywords_processed += 1
                            
                            # æ›´æ–°æ•°æ®åº“ä¸­çš„å…³é”®è¯
                            settings.docStoreConn.update(
                                {"id": d["id"]}, 
                                {"important_kwd": d["important_kwd"], "important_tks": d["important_tks"]},
                                search.index_name(tenant_id), dataset_id
                            )
                    except Exception as e:
                        print(f"Keywords extraction error: {str(e)[:100]}")
                
                async with trio.open_nursery() as nursery:
                    for d in all_processed_chunks:
                        nursery.start_soon(doc_keyword_extraction, chat_model, d, auto_keywords)
                
                print(f"[Keywords] å…¨é‡å…³é”®è¯ç”Ÿæˆå®Œæˆ: {keywords_processed}/{len(all_processed_chunks)}, è€—æ—¶ {timer() - st:.2f}s")
                
                # æ›´æ–°å…³é”®è¯ç”Ÿæˆå®Œæˆè¿›åº¦
                if keywords_processed > 0:
                    _update_progress_state(document_id, progress=0.7, 
                                         message=f"å…³é”®è¯ç”Ÿæˆå®Œæˆ: {keywords_processed}/{len(all_processed_chunks)} ä¸ªæ–‡æœ¬å—", 
                                         stage="keywords_completed")
            
            # é—®é¢˜ç”Ÿæˆå¤„ç†
            questions_processed = 0
            
            if auto_questions > 0:
                st = timer()
                
                async def doc_question_proposal(chat_mdl, d, topn):
                    nonlocal questions_processed
                    try:
                        content = d.get('content_with_weight', '').strip()
                        if not content or len(content) < 10:  # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
                            return
                        
                        # æ£€æŸ¥ç¼“å­˜
                        cached = get_llm_cache(chat_mdl.llm_name, content, "question", {"topn": topn})
                        if not cached:
                            async with chat_limiter:
                                # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°
                                cached = await trio.to_thread.run_sync(
                                    lambda: question_proposal(chat_mdl, content, topn)
                                )
                            # åªç¼“å­˜æœ‰æ•ˆç»“æœ
                            if cached and cached.strip():
                                set_llm_cache(chat_mdl.llm_name, content, cached, "question", {"topn": topn})
                        
                        if cached and cached.strip():
                            d["question_kwd"] = cached.split("\n")
                            d["question_tks"] = rag_tokenizer.tokenize("\n".join(d["question_kwd"]))
                            questions_processed += 1
                            
                            # æ›´æ–°æ•°æ®åº“ä¸­çš„é—®é¢˜
                            settings.docStoreConn.update(
                                {"id": d["id"]}, 
                                {"question_kwd": d["question_kwd"], "question_tks": d["question_tks"]},
                                search.index_name(tenant_id), dataset_id
                            )
                    except Exception as e:
                        print(f"Questions generation error: {str(e)[:100]}")
                
                async with trio.open_nursery() as nursery:
                    for d in all_processed_chunks:
                        nursery.start_soon(doc_question_proposal, chat_model, d, auto_questions)
                
                print(f"[Questions] å…¨é‡é—®é¢˜ç”Ÿæˆå®Œæˆ: {questions_processed}/{len(all_processed_chunks)}, è€—æ—¶ {timer() - st:.2f}s")
                
                # æ›´æ–°é—®é¢˜ç”Ÿæˆå®Œæˆè¿›åº¦
                if questions_processed > 0:
                    _update_progress_state(document_id, progress=0.8, 
                                         message=f"é—®é¢˜ç”Ÿæˆå®Œæˆ: {questions_processed}/{len(all_processed_chunks)} ä¸ªæ–‡æœ¬å—", 
                                         stage="questions_completed")
            
            return keywords_processed, questions_processed
        
        # è¿è¡Œå¼‚æ­¥å¤„ç†
        keywords_processed, questions_processed = trio.run(process_batch_keywords_and_questions)
        return True
        
    except Exception as e:
        print(f"[_process_auto_keywords_questions] å¤„ç†å¼‚å¸¸: {e}")
        return False


def _process_graphrag(document_id, tenant_id, dataset_id, graphrag_config):
    """
    å¤„ç†GraphRAGçŸ¥è¯†å›¾è°±æ„å»º
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸå®Œæˆå¤„ç†
    """
    try:
        # è·å–ç§Ÿæˆ·å’Œæ¨¡å‹ä¿¡æ¯
        from api.db.services.user_service import TenantService
        from api.db.services.llm_service import LLMBundle
        _, tenant = TenantService.get_by_id(tenant_id)
        chat_model = LLMBundle(tenant_id, LLMType.CHAT, tenant.llm_id)
        
        # è·å–çŸ¥è¯†åº“ä¿¡æ¯
        kb_exists, kb = KnowledgebaseService.get_by_id(dataset_id)
        if not kb_exists or not kb:
            raise RuntimeError(f"Knowledge base {dataset_id} not found")
        embedding_model = LLMBundle(tenant_id, LLMType.EMBEDDING, kb.embd_id)
        
        # æ„å»ºGraphRAGå¤„ç†å‚æ•°
        row = {
            'tenant_id': tenant_id,
            'kb_id': dataset_id,
            'doc_id': document_id,
            'kb_parser_config': {
                'graphrag': {
                    'method': graphrag_config.get('method', 'light'),
                    'entity_types': graphrag_config.get('entity_types', 
                        ['organization', 'person', 'geo', 'event', 'category']),
                    'use_graphrag': True
                }
            }
        }
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(progress=None, msg=""):
            print(f"[GraphRAG Progress] {document_id}: {msg}")
            if msg:
                # å°†GraphRAGçš„è¿›åº¦ä¿¡æ¯æ›´æ–°åˆ°çŠ¶æ€ä¸­
                _update_progress_state(document_id, progress=0.9, 
                                     message=f"çŸ¥è¯†å›¾è°±æ„å»º: {msg}", 
                                     stage="graphrag_processing")
        
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥ä¼ é€’å‚æ•°
        async def run_graphrag():
            from graphrag.general import index
            return await index.run_graphrag(
                row=row,
                language=graphrag_config.get('language', 'Chinese'),
                with_resolution=graphrag_config.get('resolution', False),
                with_community=graphrag_config.get('community', False),
                chat_model=chat_model,
                embedding_model=embedding_model,
                callback=progress_callback
            )
        
        # è°ƒç”¨GraphRAGæŠ½å–
        result = trio.run(run_graphrag)
        
        # æ›´æ–°çŸ¥è¯†å›¾è°±å®Œæˆè¿›åº¦
        _update_progress_state(document_id, progress=0.95, 
                             message="çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ", 
                             stage="graphrag_completed")
        return True
        
    except Exception as e:
        print(f"[_process_graphrag] å¤„ç†å¼‚å¸¸: {e}")
        return False


def _get_progress_stats():
    """è·å–è¿›åº¦çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰"""
    with _progress_lock:
        total_states = len(_progress_states)
        completed_states = sum(1 for state in _progress_states.values() 
                             if state.get("stage", "") in ["completed", "completed_with_errors"])
        active_states = total_states - completed_states
        
        return {
            "total_states": total_states,
            "active_states": active_states, 
            "completed_states": completed_states
        }


@manager.route(  # noqa: F821
    "/datasets/<dataset_id>/documents/<document_id>/parse/progress", methods=["GET"]
)
@token_required
def get_parse_progress(tenant_id, dataset_id, document_id):
    """
    è·å–æ–‡æ¡£è§£æè¿›åº¦
    ---
    tags:
      - Parse Progress
    security:
      - ApiKeyAuth: []
    parameters:
      - in: path
        name: dataset_id
        type: string
        required: true
        description: ID of the dataset.
      - in: path
        name: document_id
        type: string
        required: true
        description: ID of the document.
    responses:
      200:
        description: Progress information retrieved successfully.
        schema:
          type: object
          properties:
            progress:
              type: number
              description: Progress value (0.0 to 1.0).
            message:
              type: string
              description: Current progress message.
            stage:
              type: string
              description: Current processing stage.
            timestamp:
              type: number
              description: Last update timestamp.
    """
    # åŸºç¡€æƒé™éªŒè¯
    if not KnowledgebaseService.accessible(kb_id=dataset_id, user_id=tenant_id):
        return get_error_data_result(message=f"You don't own the dataset {dataset_id}.")
    
    doc = DocumentService.query(id=document_id, kb_id=dataset_id)
    if not doc:
        return get_error_data_result(message=f"You don't own the document {document_id}.")
    
    # è·å–è¿›åº¦çŠ¶æ€
    progress_state = _get_progress_state(document_id)
    
    return get_result(data=progress_state)


@manager.route(  # noqa: F821
    "/progress/stats", methods=["GET"]
)
@token_required
def get_progress_stats(tenant_id):
    """
    è·å–è¿›åº¦çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯ï¼ˆç›‘æ§æ¥å£ï¼‰
    ---
    tags:
      - Progress Stats
    security:
      - ApiKeyAuth: []
    responses:
      200:
        description: Progress statistics retrieved successfully.
        schema:
          type: object
          properties:
            total_states:
              type: integer
              description: Total number of progress states in memory.
            active_states:
              type: integer
              description: Number of active (non-completed) states.
            completed_states:
              type: integer
              description: Number of completed states awaiting cleanup.
    """
    stats = _get_progress_stats()
    return get_result(data=stats)


def _add_positions_to_chunk_data(d, positions):
    """
    ç®€åŒ–ç‰ˆæœ¬ï¼šä»…æ·»åŠ position_intä¿¡æ¯ï¼Œä¸è¦†ç›–page_num_intå’Œtop_intæ’åºå­—æ®µ
    Args:
        d: chunk data dictionary
        positions: list of [page_num, left, right, top, bottom] tuples
    """
    if not positions:
        return
    
    position_int = []
    
    for pos in positions:
        if len(pos) != 5:
            continue  # Skip invalid positions
            
        pn, left, right, top, bottom = pos
        # ä½¿ç”¨å…ƒç»„æ ¼å¼ï¼Œä¸åŸå§‹RAGFlowä¿æŒä¸€è‡´
        position_int.append((int(pn + 1), int(left), int(right), int(top), int(bottom)))
    
    if position_int:  # Only add if we have valid positions
        # ä»…æ·»åŠ ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼Œä¸ä¿®æ”¹æ’åºå­—æ®µ
        d["position_int"] = position_int





@manager.route(  # noqa: F821
    "/datasets/<dataset_id>/documents/<document_id>/chunks/batch", methods=["POST"]
)
@token_required
def batch_add_chunk(tenant_id, dataset_id, document_id):
    """
    Add multiple chunks to a document in batch.
    ---
    tags:
      - Chunks
    security:
      - ApiKeyAuth: []
    parameters:
      - in: path
        name: dataset_id
        type: string
        required: true
        description: ID of the dataset.
      - in: path
        name: document_id
        type: string
        required: true
        description: ID of the document.
      - in: body
        name: body
        description: Batch chunk data.
        required: true
        schema:
          type: object
          properties:
            chunks:
              type: array
              items:
                type: object
                properties:
                  content:
                    type: string
                    required: true
                    description: Content of the chunk.
                  important_keywords:
                    type: array
                    items:
                      type: string
                    description: Important keywords.
                  questions:
                    type: array
                    items:
                      type: string
                    description: Questions related to the chunk.
                  positions:
                    type: array
                    items:
                      type: array
                      items:
                        type: integer
                      minItems: 5
                      maxItems: 5
                    description: Position information as list of [page_num, left, right, top, bottom].
              required: true
              description: Array of chunks to add.
            batch_size:
              type: integer
              description: Size of each processing batch (default 10, max 50).
            max_retries:
              type: integer
              description: Maximum number of retries for failed batches (default 3, max 5).
            retry_delay:
              type: number
              description: Delay in seconds between retries (default 2.0, max 10.0).
      - in: header
        name: Authorization
        type: string
        required: true
        description: Bearer token for authentication.
    responses:
      200:
        description: Chunks added successfully.
        schema:
          type: object
          properties:
            chunks:
              type: array
              items:
                type: object
                properties:
                  id:
                    type: string
                    description: Chunk ID.
                  content:
                    type: string
                    description: Chunk content.
                  document_id:
                    type: string
                    description: ID of the document.
                  important_keywords:
                    type: array
                    items:
                      type: string
                    description: Important keywords.
                  positions:
                    type: array
                    items:
                      type: array
                      items:
                        type: integer
                    description: Position information.
            total_added:
              type: integer
              description: Total number of chunks successfully added.
            total_failed:
              type: integer
              description: Total number of chunks that failed to add.
    """
    # é…ç½®å‚æ•°
    MAX_CHUNKS_PER_REQUEST = 100
    DEFAULT_BATCH_SIZE = 10
    MAX_BATCH_SIZE = 50
    MAX_CONTENT_LENGTH = 10000
    DB_BULK_SIZE = 10
    
    try:
        # ===== 1. æƒé™å’ŒåŸºç¡€éªŒè¯ =====
        if not KnowledgebaseService.accessible(kb_id=dataset_id, user_id=tenant_id):
            return get_error_data_result(message=f"You don't own the dataset {dataset_id}.")
        
        doc = DocumentService.query(id=document_id, kb_id=dataset_id)
        if not doc:
            return get_error_data_result(message=f"You don't own the document {document_id}.")
        doc = doc[0]
        
        # ===== 2. è¯·æ±‚æ•°æ®è§£æå’ŒéªŒè¯ =====
        req = request.json
        chunks_data = req.get("chunks", [])
        batch_size = min(req.get("batch_size", DEFAULT_BATCH_SIZE), MAX_BATCH_SIZE)
        
        # é‡è¯•æœºåˆ¶é…ç½®ï¼ˆå¯é€šè¿‡è¯·æ±‚å‚æ•°è°ƒæ•´ï¼‰
        MAX_RETRIES = min(req.get("max_retries", 3), 5)  # é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°ä¸º5
        RETRY_DELAY = min(req.get("retry_delay", 2.0), 10.0)  # é™åˆ¶æœ€å¤§å»¶è¿Ÿä¸º10ç§’
        
        # åŸºç¡€æ•°æ®éªŒè¯
        if not chunks_data or not isinstance(chunks_data, list):
            return get_error_data_result(message="`chunks` is required and must be a list")
        
        if len(chunks_data) == 0:
            return get_error_data_result(message="No chunks provided")
        
        if len(chunks_data) > MAX_CHUNKS_PER_REQUEST:
            return get_error_data_result(
                message=f"Too many chunks. Maximum allowed: {MAX_CHUNKS_PER_REQUEST}, received: {len(chunks_data)}"
            )
        
        # ===== 3. æ•°æ®éªŒè¯ =====
        validated_chunks = []
        validation_errors = []
        
        for i, chunk_req in enumerate(chunks_data):
            # å†…å®¹éªŒè¯
            content = str(chunk_req.get("content", "")).strip()
            if not content:
                validation_errors.append(f"Chunk {i}: content is required")
                continue
                
            if len(content) > MAX_CONTENT_LENGTH:
                validation_errors.append(f"Chunk {i}: content too long ({len(content)} chars, max {MAX_CONTENT_LENGTH})")
                continue
            
            # å…³é”®è¯å’Œé—®é¢˜éªŒè¯    
            if "important_keywords" in chunk_req and not isinstance(chunk_req["important_keywords"], list):
                validation_errors.append(f"Chunk {i}: important_keywords must be a list")
                continue
                    
            if "questions" in chunk_req and not isinstance(chunk_req["questions"], list):
                validation_errors.append(f"Chunk {i}: questions must be a list")
                continue
            
            # ä½ç½®ä¿¡æ¯éªŒè¯
            if "positions" in chunk_req:
                positions = chunk_req["positions"]
                if not isinstance(positions, list):
                    validation_errors.append(f"Chunk {i}: positions must be a list")
                    continue
                
                for j, pos in enumerate(positions):
                    if not isinstance(pos, list) or len(pos) != 5:
                        validation_errors.append(f"Chunk {i}: positions[{j}] must be a list of 5 integers [page_num, left, right, top, bottom]")
                        break
                    
                    try:
                        [int(x) for x in pos]
                    except (ValueError, TypeError):
                        validation_errors.append(f"Chunk {i}: positions[{j}] must contain only integers")
                        break
                
                if validation_errors and validation_errors[-1].startswith(f"Chunk {i}:"):
                    continue
            
            validated_chunks.append((i, chunk_req))
        
        # éªŒè¯é”™è¯¯å¤„ç†
        if validation_errors:
            error_msg = "; ".join(validation_errors[:10])
            if len(validation_errors) > 10:
                error_msg += f" ... and {len(validation_errors)-10} more errors"
            # æ ‡è®°ä¸ºéªŒè¯å¤±è´¥çŠ¶æ€
            _update_progress_state(document_id, progress=-1, message=f"æ•°æ®éªŒè¯å¤±è´¥: {error_msg}", stage="validation_failed")
            return get_error_data_result(message=f"Validation errors: {error_msg}")
        
        # ===== 4. åˆå§‹åŒ– embedding æ¨¡å‹ =====
        try:
            embd_id = DocumentService.get_embd_id(document_id)
            embd_mdl = TenantLLMService.model_instance(tenant_id, LLMType.EMBEDDING.value, embd_id)
        except Exception as e:
            # æ ‡è®°ä¸ºåˆå§‹åŒ–å¤±è´¥çŠ¶æ€
            _update_progress_state(document_id, progress=-1, message=f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}", stage="initialization_failed")
            return get_error_data_result(message=f"Failed to initialize embedding model: {str(e)}")
        
        # ===== 5. æ‰¹é‡å¤„ç†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰ =====
        all_processed_chunks = []
        total_cost = 0
        processing_errors = []
        current_time = str(datetime.datetime.now()).replace("T", " ")[:19]
        current_timestamp = datetime.datetime.now().timestamp()
        
        # é‡è¯•é…ç½®ï¼ˆå·²ä»è¯·æ±‚å‚æ•°ä¸­è·å–ï¼‰
        
        print(f"[batch_add_chunk] è¯·æ±‚: dataset_id={dataset_id}, document_id={document_id}, chunks={len(chunks_data)}")
        
        # åˆå§‹åŒ–è¿›åº¦çŠ¶æ€
        _update_progress_state(document_id, progress=0.1, message="å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æœ¬å—...", stage="initializing")
        
        # å¤±è´¥æ‰¹æ¬¡è·Ÿè¸ªï¼ˆä¿æŒé¡ºåºï¼‰
        failed_batches = []  # å­˜å‚¨ (batch_index, batch_chunks, retry_count) 
        
        # ç¬¬ä¸€è½®å¤„ç†ï¼šæŒ‰é¡ºåºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        for batch_index in range(0, len(validated_chunks), batch_size):
            batch_end = min(batch_index + batch_size, len(validated_chunks))
            batch_chunks = validated_chunks[batch_index:batch_end]
            
            print(f"[batch_add_chunk] å¤„ç†batch: {batch_index}~{batch_end}")
            
            # æ›´æ–°åˆ†å—å¤„ç†è¿›åº¦
            chunk_progress = 0.1 + (batch_index / len(validated_chunks)) * 0.35  # 0.1-0.45 ç”¨äºç¬¬ä¸€è½®å¤„ç†
            _update_progress_state(document_id, progress=chunk_progress, 
                                 message=f"å¤„ç†æ–‡æœ¬å— {batch_index+1}-{batch_end}/{len(validated_chunks)}", 
                                 stage="chunking")
            
            success, batch_result = _process_single_batch(
                batch_chunks, batch_index, embd_mdl, doc, dataset_id, document_id, 
                current_time, current_timestamp, tenant_id, DB_BULK_SIZE
            )
            
            if success:
                all_processed_chunks.extend(batch_result["processed_chunks"])
                total_cost += batch_result["cost"]
            else:
                # è®°å½•å¤±è´¥çš„æ‰¹æ¬¡ï¼Œç¨åé‡è¯•
                failed_batches.append((batch_index, batch_chunks, 0))
                processing_errors.append(f"Batch {batch_index//batch_size + 1} failed: {batch_result.get('error', 'Unknown error')}")
                print(f"[batch_add_chunk] Batch {batch_index//batch_size + 1} å¤±è´¥ï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—: {batch_result.get('error', 'Unknown error')}")
        
        # é‡è¯•å¤±è´¥çš„æ‰¹æ¬¡ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        if failed_batches:
            print(f"[batch_add_chunk] å¼€å§‹é‡è¯• {len(failed_batches)} ä¸ªå¤±è´¥çš„æ‰¹æ¬¡...")
            _update_progress_state(document_id, progress=0.5, 
                                 message=f"å¼€å§‹é‡è¯• {len(failed_batches)} ä¸ªå¤±è´¥æ‰¹æ¬¡...", 
                                 stage="retrying")
            
            # æŒ‰æ‰¹æ¬¡ç´¢å¼•æ’åºï¼Œç¡®ä¿æŒ‰åŸå§‹é¡ºåºé‡è¯•
            failed_batches.sort(key=lambda x: x[0])
            
            retry_round = 1
            while failed_batches and retry_round <= MAX_RETRIES:
                print(f"[batch_add_chunk] é‡è¯•è½®æ¬¡ {retry_round}/{MAX_RETRIES}")
                
                remaining_failed = []
                for batch_index, batch_chunks, prev_retry_count in failed_batches:
                    batch_num = batch_index // batch_size + 1
                    
                    # æ›´æ–°é‡è¯•è¿›åº¦
                    retry_progress = 0.5 + (retry_round - 1) * 0.15  # 0.5-0.95 ç”¨äºé‡è¯•
                    _update_progress_state(document_id, progress=retry_progress,
                                         message=f"é‡è¯•æ‰¹æ¬¡ {batch_num} (ç¬¬{retry_round}æ¬¡é‡è¯•)", 
                                         stage="retrying")
                    
                    # é‡è¯•å‰ç­‰å¾…
                    time.sleep(RETRY_DELAY)
                    
                    success, batch_result = _process_single_batch(
                        batch_chunks, batch_index, embd_mdl, doc, dataset_id, document_id, 
                        current_time, current_timestamp, tenant_id, DB_BULK_SIZE
                    )
                    
                    if success:
                        print(f"[batch_add_chunk] Batch {batch_num} é‡è¯•æˆåŠŸï¼")
                        all_processed_chunks.extend(batch_result["processed_chunks"])
                        total_cost += batch_result["cost"]
                    else:
                        print(f"[batch_add_chunk] Batch {batch_num} é‡è¯•å¤±è´¥ (ç¬¬{retry_round}æ¬¡): {batch_result.get('error', 'Unknown error')}")
                        remaining_failed.append((batch_index, batch_chunks, retry_round))
                
                failed_batches = remaining_failed
                retry_round += 1
        
        # å¯¹æˆåŠŸå¤„ç†çš„chunksæŒ‰åŸå§‹é¡ºåºæ’åº
        # ä½¿ç”¨ top_int å­—æ®µè¿›è¡Œæ’åºï¼Œç¡®ä¿æœ€ç»ˆé¡ºåºæ­£ç¡®
        all_processed_chunks.sort(key=lambda x: x.get('top_int', 0))
        
        if failed_batches:
            final_failed_count = sum(len(batch[1]) for batch in failed_batches)
            print(f"[batch_add_chunk] æœ€ç»ˆä»æœ‰ {len(failed_batches)} ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œå…± {final_failed_count} ä¸ªchunks")
            
            for batch_index, batch_chunks, _ in failed_batches:
                batch_num = batch_index // batch_size + 1 
                processing_errors.append(f"Batch {batch_num} failed after {MAX_RETRIES} retries ({len(batch_chunks)} chunks)")
        
        # ===== 6. å¤„ç†è‡ªåŠ¨å…³é”®è¯å’Œé—®é¢˜ç”Ÿæˆ =====
        if all_processed_chunks:
            try:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨å…³é”®è¯/é—®é¢˜
                exists, doc_for_auto_gen = DocumentService.get_by_id(document_id)
                if exists and doc_for_auto_gen and doc_for_auto_gen.parser_config:
                    import json
                    if isinstance(doc_for_auto_gen.parser_config, str):
                        parser_config = json.loads(doc_for_auto_gen.parser_config)
                    else:
                        parser_config = doc_for_auto_gen.parser_config
                    
                    auto_keywords = parser_config.get('auto_keywords', 0)
                    auto_questions = parser_config.get('auto_questions', 0)
                    
                    if auto_keywords > 0 or auto_questions > 0:
                        print(f"[Keywords/Questions] å…¨é‡å¤„ç† - keywords: {auto_keywords}, questions: {auto_questions}, chunks: {len(all_processed_chunks)}")
                        
                        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹å…³é”®è¯å’Œé—®é¢˜ç”Ÿæˆ
                        _update_progress_state(document_id, progress=0.6, 
                                             message="å¼€å§‹ç”Ÿæˆè‡ªåŠ¨å…³é”®è¯å’Œé—®é¢˜...", 
                                             stage="keywords_questions")
                        
                        # è·å–ç§Ÿæˆ·ä¿¡æ¯å’ŒLLMæ¨¡å‹
                        from api.db.services.user_service import TenantService
                        from api.db.services.llm_service import LLMBundle
                        _, tenant = TenantService.get_by_id(tenant_id)
                        chat_model = LLMBundle(tenant_id, LLMType.CHAT, tenant.llm_id)
                        
                        # æ‰¹é‡å¤„ç†è‡ªåŠ¨å…³é”®è¯å’Œé—®é¢˜ç”Ÿæˆ
                        success = _process_auto_keywords_questions(
                            all_processed_chunks, document_id, chat_model, 
                            auto_keywords, auto_questions, tenant_id, dataset_id
                        )
                        
                        if success:
                            print(f"[Keywords/Questions] å…¨é‡å¤„ç†å®Œæˆ")
                        else:
                            print(f"[Keywords/Questions] å…¨é‡å¤„ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸»æµç¨‹")
                            
            except Exception as auto_gen_e:
                print(f"[Keywords/Questions] å…¨é‡å¤„ç†å¼‚å¸¸: {auto_gen_e}")
                # ç»§ç»­å¤„ç†ï¼Œä¸å› ä¸ºè‡ªåŠ¨ç”Ÿæˆå¤±è´¥è€Œä¸­æ–­ä¸»æµç¨‹
        
        # ===== 7. å¤„ç†GraphRAG =====
        if all_processed_chunks:
            try:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨GraphRAG
                exists, doc_for_graphrag = DocumentService.get_by_id(document_id)
                if exists and doc_for_graphrag and doc_for_graphrag.parser_config:
                    import json
                    if isinstance(doc_for_graphrag.parser_config, str):
                        parser_config = json.loads(doc_for_graphrag.parser_config)
                    else:
                        parser_config = doc_for_graphrag.parser_config
                    
                    graphrag_config = parser_config.get('graphrag', {})
                    if graphrag_config.get('use_graphrag', False):
                        print(f"[GraphRAG] å…¨é‡å¤„ç† - å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} æŠ½å–çŸ¥è¯†å›¾è°±ï¼Œchunks: {len(all_processed_chunks)}")
                        
                        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹çŸ¥è¯†å›¾è°±æ„å»º
                        _update_progress_state(document_id, progress=0.85, 
                                             message="å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...", 
                                             stage="graphrag_processing")
                        
                        success = _process_graphrag(
                            document_id, tenant_id, dataset_id, graphrag_config
                        )
                        
                        if success:
                            print(f"[GraphRAG] å…¨é‡å¤„ç†å®Œæˆ - æ–‡æ¡£ {document_id} çŸ¥è¯†å›¾è°±æŠ½å–æˆåŠŸ")
                        else:
                            print(f"[GraphRAG] å…¨é‡å¤„ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸»æµç¨‹")
                            
            except Exception as graphrag_e:
                print(f"[GraphRAG] å…¨é‡å¤„ç†å¼‚å¸¸ {document_id}: {graphrag_e}")
                # GraphRAGå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­å¤„ç†
        
        # ===== 6. æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ =====
        if all_processed_chunks:
            try:
                DocumentService.increment_chunk_num(doc.id, doc.kb_id, total_cost, len(all_processed_chunks), 0)
            except Exception as e:
                print(f"Warning: Failed to update document count: {e}")
        
        # ===== 7. æ ¼å¼åŒ–å“åº”æ•°æ® =====
        key_mapping = {
            "id": "id",
            "content_with_weight": "content",
            "doc_id": "document_id",
            "important_kwd": "important_keywords",
            "question_kwd": "questions",
            "kb_id": "dataset_id",
            "create_timestamp_flt": "create_timestamp",
            "create_time": "create_time",
            "position_int": "positions",
            "image_id": "image_id",
            "available_int": "available",
        }

        renamed_chunks = []
        for d in all_processed_chunks:
            renamed_chunk = {}
            for key, value in d.items():
                if key in key_mapping:
                    new_key = key_mapping[key]
                    # å°†position_intçš„å…ƒç»„æ ¼å¼è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                    if key == "position_int" and isinstance(value, list):
                        renamed_chunk[new_key] = [list(pos) if isinstance(pos, tuple) else pos for pos in value]
                    else:
                        renamed_chunk[new_key] = value
            
            # ç¡®ä¿æ¯ä¸ªchunkéƒ½æœ‰positionså­—æ®µ
            if "positions" not in renamed_chunk:
                renamed_chunk["positions"] = []
            
            renamed_chunks.append(renamed_chunk)
        
        # ===== 7. åˆå§‹åŒ–GraphRAGç»“æœ =====
        # æ³¨æ„ï¼šå®é™…çš„GraphRAGå¤„ç†å·²åœ¨æ‰¹å¤„ç†ä¸­å®Œæˆ
        graphrag_result = {
            'status': 'success',
            'doc_id': document_id,
            'message': 'çŸ¥è¯†å›¾è°±æŠ½å–å·²åœ¨æ‰¹å¤„ç†ä¸­å®Œæˆ'
        }
        
        # ===== 8. åˆå§‹åŒ–è‡ªåŠ¨å…³é”®è¯å’Œé—®é¢˜ç”Ÿæˆç»“æœ =====
        keywords_result = {
            'status': 'success',
            'processed_chunks': 0,
            'message': 'å…³é”®è¯ç”Ÿæˆå·²åœ¨æ‰¹å¤„ç†ä¸­å®Œæˆ'
        }
        
        questions_result = {
            'status': 'success',
            'processed_chunks': 0,
            'message': 'é—®é¢˜ç”Ÿæˆå·²åœ¨æ‰¹å¤„ç†ä¸­å®Œæˆ'
        }
        
        # ===== 9. æ„å»ºè¿”å›ç»“æœ =====
        total_requested = len(chunks_data)
        total_added = len(renamed_chunks)
        total_failed = total_requested - total_added
        
        result_data = {
            "chunks": renamed_chunks,
            "total_added": total_added,
            "total_failed": total_failed,
            "processing_stats": {
                "total_requested": total_requested,
                "batch_size_used": batch_size,
                "batches_processed": (len(validated_chunks) - 1) // batch_size + 1,
                "embedding_cost": total_cost,
                "processing_errors": processing_errors if processing_errors else None
            },
            "graphrag_result": graphrag_result,  # GraphRAGå¤„ç†ç»“æœ
            "keywords_result": keywords_result,  # å…³é”®è¯æå–ç»“æœ
            "questions_result": questions_result  # å…³é”®é—®é¢˜ç”Ÿæˆç»“æœ
        }
        
        # æ›´æ–°æœ€ç»ˆå®ŒæˆçŠ¶æ€
        if total_failed == 0:
            _update_progress_state(document_id, progress=1.0, 
                                 message=f"å¤„ç†å®Œæˆï¼æˆåŠŸæ·»åŠ  {total_added} ä¸ªæ–‡æœ¬å—", 
                                 stage="completed")
        else:
            _update_progress_state(document_id, progress=1.0, 
                                 message=f"å¤„ç†å®Œæˆï¼æˆåŠŸ {total_added} ä¸ªï¼Œå¤±è´¥ {total_failed} ä¸ª", 
                                 stage="completed_with_errors")
        
        # è¿”å›ç»“æœ
        if processing_errors:
            return get_result(
                data=result_data,
                message=f"Partial success: {total_added} chunks added, {total_failed} failed. Check processing_stats for details."
            )
        else:
            return get_result(data=result_data)
            
    except Exception as e:
        # å¤„ç†æ„å¤–å¼‚å¸¸
        _update_progress_state(document_id, progress=-1, message=f"å¤„ç†å¤±è´¥: {str(e)}", stage="failed")
        return get_error_data_result(message=f"Batch processing failed: {str(e)}")
    
    finally:
        # è¯·æ±‚ç»“æŸåç›´æ¥æ¸…ç©ºè¿›åº¦çŠ¶æ€
        _clear_progress_state(document_id)

# è®¾ç½®é¡µé¢åç§° (å¯é€‰ï¼Œç”¨äºè‡ªå®šä¹‰ URL å‰ç¼€)
page_name = "batch_chunk"