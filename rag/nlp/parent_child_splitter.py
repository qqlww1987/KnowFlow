#
#  Copyright 2025 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

"""
åŸºäº Smart AST åˆ†å—ç­–ç•¥çš„çˆ¶å­æ–‡æ¡£åˆ†å‰²å™¨
æ•´åˆç°æœ‰çš„æ™ºèƒ½åˆ†å—èƒ½åŠ›ï¼Œå®ç°çˆ¶å­å±‚çº§ç»“æ„
"""

import hashlib
import json
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# å¯¼å…¥ç°æœ‰çš„æ™ºèƒ½åˆ†å—æ¨¡å—
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'knowflow', 'server', 'services', 'knowledgebases', 'mineru_parse'))

try:
    from utils import (
        split_markdown_to_chunks_smart,
        num_tokens_from_string,
        get_configured_chunk_method
    )
    SMART_CHUNKING_AVAILABLE = True
    print("[INFO] Successfully imported Smart chunking utilities.")
except ImportError:
    print("[WARNING] Could not import Smart chunking utilities. Using fallback methods.")
    SMART_CHUNKING_AVAILABLE = False
    
    # ä½¿ç”¨tiktokenè¿›è¡Œå‡†ç¡®çš„tokenè®¡ç®—
    try:
        import tiktoken
        encoder = tiktoken.get_encoding("cl100k_base")
        
        def num_tokens_from_string(text):
            """ä½¿ç”¨tiktokenè¿›è¡Œå‡†ç¡®tokenè®¡ç®—"""
            return len(encoder.encode(text))
    except ImportError:
        def num_tokens_from_string(text):
            """ç®€å•tokenè®¡æ•°å›é€€"""
            # å¯¹ä¸­æ–‡æ–‡æœ¬çš„æ”¹è¿›ä¼°ç®—
            return len(text) // 2 if any('\u4e00' <= char <= '\u9fff' for char in text) else len(text.split())
    
    def split_markdown_to_chunks_smart(txt, chunk_token_num=256, min_chunk_tokens=10):
        """å›é€€æ–¹æ³•ï¼šç®€å•æŒ‰æ®µè½åˆ†å‰²"""
        if not txt or not txt.strip():
            return []
        
        paragraphs = [p.strip() for p in txt.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = num_tokens_from_string(para)
            if current_tokens + para_tokens > chunk_token_num and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_tokens = para_tokens
            else:
                current_chunk.append(para)
                current_tokens += para_tokens
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks


@dataclass
class ChunkInfo:
    """åˆ†å—ä¿¡æ¯æ•°æ®ç±»"""
    id: str
    content: str
    token_count: int
    char_count: int
    order: int
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ParentChildResult:
    """çˆ¶å­åˆ†å—ç»“æœ"""
    parent_chunks: List[ChunkInfo]
    child_chunks: List[ChunkInfo]
    relationships: List[Dict[str, Any]]
    total_parents: int
    total_children: int


class SmartParentChildSplitter:
    """åŸºäº Smart AST åˆ†å—çš„çˆ¶å­æ–‡æ¡£åˆ†å‰²å™¨"""
    
    def __init__(self, 
                 parent_chunk_size: int = 1024,
                 child_chunk_size: int = 256,
                 parent_overlap: int = 100,
                 child_overlap: int = 50,
                 min_child_size: int = 10,
                 parent_separator: str = r'\n\n',
                 child_separator: str = r'[ã€‚ï¼ï¼Ÿ.!?]'):
        """
        åˆå§‹åŒ–åˆ†å‰²å™¨
        
        Args:
            parent_chunk_size: çˆ¶åˆ†å—å¤§å°ï¼ˆtokensï¼‰
            child_chunk_size: å­åˆ†å—å¤§å°ï¼ˆtokensï¼‰
            parent_overlap: çˆ¶åˆ†å—é‡å ï¼ˆtokensï¼‰
            child_overlap: å­åˆ†å—é‡å ï¼ˆtokensï¼‰
            min_child_size: æœ€å°å­åˆ†å—å¤§å°
            parent_separator: çˆ¶åˆ†å—åˆ†éš”ç¬¦æ­£åˆ™
            child_separator: å­åˆ†å—åˆ†éš”ç¬¦æ­£åˆ™
        """
        self.parent_chunk_size = parent_chunk_size
        self.child_chunk_size = child_chunk_size
        self.parent_overlap = parent_overlap
        self.child_overlap = child_overlap
        self.min_child_size = min_child_size
        self.parent_separator = parent_separator
        self.child_separator = child_separator
        
    def split_text(self, 
                   text: str, 
                   doc_id: str,
                   kb_id: str,
                   metadata: Optional[Dict[str, Any]] = None) -> ParentChildResult:
        """
        æ‰§è¡Œçˆ¶å­åˆ†å—
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            doc_id: æ–‡æ¡£ID
            kb_id: çŸ¥è¯†åº“ID
            metadata: é¢å¤–çš„å…ƒæ•°æ®
            
        Returns:
            ParentChildResult: çˆ¶å­åˆ†å—ç»“æœ
        """
        if not text or not text.strip():
            return ParentChildResult([], [], [], 0, 0)
        
        metadata = metadata or {}
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨Smartåˆ†å—è·å¾—åŸºç¡€åˆ†å—
        base_chunks = split_markdown_to_chunks_smart(
            text, 
            chunk_token_num=self.child_chunk_size,
            min_chunk_tokens=self.min_child_size
        )
        
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨è°ƒè¯•
        if os.environ.get("DEBUG_PARENT_CHILD", "").lower() == "true":
            print(f"ğŸ“Š [DEBUG] Smartåˆ†å—ç»“æœ: {len(base_chunks)} ä¸ªåŸºç¡€åˆ†å—")
            for i, chunk in enumerate(base_chunks):
                print(f"  åŸºç¡€åˆ†å—{i+1}: {num_tokens_from_string(chunk)} tokens - {chunk[:50]}...")
        
        if not base_chunks:
            return ParentChildResult([], [], [], 0, 0)
        
        # ç¬¬äºŒæ­¥ï¼šæ„å»ºçˆ¶å­å±‚çº§ç»“æ„
        parent_chunks, child_chunks, relationships = self._build_parent_child_hierarchy(
            base_chunks, doc_id, kb_id, metadata
        )
        
        return ParentChildResult(
            parent_chunks=parent_chunks,
            child_chunks=child_chunks,
            relationships=relationships,
            total_parents=len(parent_chunks),
            total_children=len(child_chunks)
        )
    
    def _build_parent_child_hierarchy(self, 
                                     base_chunks: List[str], 
                                     doc_id: str, 
                                     kb_id: str, 
                                     metadata: Dict[str, Any]) -> Tuple[List[ChunkInfo], List[ChunkInfo], List[Dict[str, Any]]]:
        """æ„å»ºçˆ¶å­åˆ†å—å±‚çº§ç»“æ„"""
        
        parent_chunks = []
        child_chunks = []
        relationships = []
        
        # ç›´æ¥ä½¿ç”¨Smartåˆ†å—çš„ç»“æœä½œä¸ºå­åˆ†å—ï¼Œé¿å…é‡å¤å¤„ç†
        child_order_global = 0
        current_parent_content = []
        current_parent_tokens = 0
        current_child_ids = []
        parent_order = 0
        
        for base_chunk in base_chunks:
            chunk_tokens = num_tokens_from_string(base_chunk)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„çˆ¶åˆ†å—
            if (current_parent_tokens + chunk_tokens > self.parent_chunk_size 
                and current_parent_content):
                
                # å®Œæˆå½“å‰çˆ¶åˆ†å—
                parent_chunk = self._create_parent_chunk(
                    current_parent_content, parent_order, doc_id, kb_id, metadata
                )
                parent_chunks.append(parent_chunk)
                
                # å»ºç«‹å…³ç³»æ˜ å°„
                for child_id in current_child_ids:
                    relationships.append({
                        'child_chunk_id': child_id,
                        'parent_chunk_id': parent_chunk.id,
                        'doc_id': doc_id,
                        'kb_id': kb_id,
                        'relevance_score': 100
                    })
                
                # é‡ç½®çŠ¶æ€å¼€å§‹æ–°çš„çˆ¶åˆ†å—
                current_parent_content = []
                current_parent_tokens = 0
                current_child_ids = []
                parent_order += 1
            
            # åˆ›å»ºå­åˆ†å—ï¼ˆç›´æ¥ä½¿ç”¨Smartåˆ†å—ç»“æœï¼‰
            child_chunk = self._create_child_chunk(
                base_chunk, child_order_global, len(current_child_ids), 
                doc_id, kb_id, metadata
            )
            child_chunks.append(child_chunk)
            current_child_ids.append(child_chunk.id)
            
            # æ·»åŠ åˆ°å½“å‰çˆ¶åˆ†å—
            current_parent_content.append(base_chunk)
            current_parent_tokens += chunk_tokens
            child_order_global += 1
        
        # å¤„ç†æœ€åä¸€ä¸ªçˆ¶åˆ†å—
        if current_parent_content:
            parent_chunk = self._create_parent_chunk(
                current_parent_content, parent_order, doc_id, kb_id, metadata
            )
            parent_chunks.append(parent_chunk)
            
            # å»ºç«‹å…³ç³»æ˜ å°„
            for child_id in current_child_ids:
                relationships.append({
                    'child_chunk_id': child_id,
                    'parent_chunk_id': parent_chunk.id,
                    'doc_id': doc_id,
                    'kb_id': kb_id,
                    'relevance_score': 100
                })
        
        return parent_chunks, child_chunks, relationships
    
    def _create_parent_chunk(self, 
                           content_parts: List[str], 
                           order: int, 
                           doc_id: str, 
                           kb_id: str, 
                           metadata: Dict[str, Any]) -> ChunkInfo:
        """åˆ›å»ºçˆ¶åˆ†å—"""
        
        # åˆå¹¶å†…å®¹
        content = "\n\n".join(content_parts).strip()
        
        # ç”ŸæˆID
        chunk_id = self._generate_chunk_id(doc_id, "parent", order, content)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        token_count = num_tokens_from_string(content)
        char_count = len(content)
        
        # å¢å¼ºå…ƒæ•°æ®
        chunk_metadata = {
            **metadata,
            'chunk_type': 'parent',
            'contains_children': len(content_parts),
            'chunk_method': 'parent_smart'
        }
        
        return ChunkInfo(
            id=chunk_id,
            content=content,
            token_count=token_count,
            char_count=char_count,
            order=order,
            metadata=chunk_metadata
        )
    
    def _create_child_chunk(self, 
                          content: str, 
                          global_order: int, 
                          parent_order: int,
                          doc_id: str, 
                          kb_id: str, 
                          metadata: Dict[str, Any]) -> ChunkInfo:
        """åˆ›å»ºå­åˆ†å—"""
        
        # ç”ŸæˆID
        chunk_id = self._generate_chunk_id(doc_id, "child", global_order, content)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        token_count = num_tokens_from_string(content)
        char_count = len(content)
        
        # å¢å¼ºå…ƒæ•°æ®
        chunk_metadata = {
            **metadata,
            'chunk_type': 'child',
            'parent_order': parent_order,
            'chunk_method': 'child_smart'
        }
        
        return ChunkInfo(
            id=chunk_id,
            content=content,
            token_count=token_count,
            char_count=char_count,
            order=global_order,
            metadata=chunk_metadata
        )
    
    def _generate_chunk_id(self, doc_id: str, chunk_type: str, order: int, content: str) -> str:
        """ç”Ÿæˆåˆ†å—ID"""
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        return f"{doc_id}_{chunk_type}_{order:04d}_{content_hash}"
    
    def _further_split_child(self, content: str) -> List[str]:
        """è¿›ä¸€æ­¥åˆ†å‰²å­åˆ†å—ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        if num_tokens_from_string(content) <= self.child_chunk_size:
            return [content]
        
        # é¦–å…ˆå°è¯•æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        chunks = []
        current_chunk_parts = []
        current_tokens = 0
        
        for paragraph in paragraphs:
            paragraph_tokens = num_tokens_from_string(paragraph)
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…å‡ºäº†é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if paragraph_tokens > self.child_chunk_size:
                # å…ˆæ·»åŠ å½“å‰ç§¯ç´¯çš„éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
                if current_chunk_parts:
                    chunks.append('\n\n'.join(current_chunk_parts).strip())
                    current_chunk_parts = []
                    current_tokens = 0
                
                # å¯¹è¶…é•¿æ®µè½æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', paragraph)
                sentences = [s.strip() for s in sentences if s.strip()]
                
                sentence_chunk_parts = []
                sentence_tokens = 0
                
                for sentence in sentences:
                    sentence_token_count = num_tokens_from_string(sentence)
                    
                    if (sentence_tokens + sentence_token_count > self.child_chunk_size 
                        and sentence_chunk_parts):
                        chunks.append('ã€‚'.join(sentence_chunk_parts) + 'ã€‚')
                        sentence_chunk_parts = [sentence]
                        sentence_tokens = sentence_token_count
                    else:
                        sentence_chunk_parts.append(sentence)
                        sentence_tokens += sentence_token_count
                
                if sentence_chunk_parts:
                    chunks.append('ã€‚'.join(sentence_chunk_parts) + 'ã€‚')
                    
            else:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ åˆ°å½“å‰åˆ†å—
                if (current_tokens + paragraph_tokens > self.child_chunk_size 
                    and current_chunk_parts):
                    chunks.append('\n\n'.join(current_chunk_parts).strip())
                    current_chunk_parts = [paragraph]
                    current_tokens = paragraph_tokens
                else:
                    current_chunk_parts.append(paragraph)
                    current_tokens += paragraph_tokens
        
        # æ·»åŠ æœ€åçš„åˆ†å—
        if current_chunk_parts:
            chunks.append('\n\n'.join(current_chunk_parts).strip())
        
        # ç¡®ä¿è¿”å›çš„åˆ†å—éƒ½ä¸ä¸ºç©º
        return [chunk for chunk in chunks if chunk.strip()]


class ParentChildConfigManager:
    """çˆ¶å­åˆ†å—é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.default_config = {
            'parent_chunk_size': 1024,
            'child_chunk_size': 256,
            'parent_overlap': 100,
            'child_overlap': 50,
            'min_child_size': 10,
            'parent_separator': r'\n\n',
            'child_separator': r'[ã€‚ï¼ï¼Ÿ.!?]',
            'retrieval_mode': 'parent',
            'top_k_children': 10,
            'top_k_parents': 4
        }
    
    def get_config(self, kb_id: str) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“çš„çˆ¶å­åˆ†å—é…ç½®"""
        try:
            from api.db.parent_child_models import ParentChildConfig
            config = ParentChildConfig.get_by_id(kb_id)
            
            return {
                'parent_chunk_size': config.parent_chunk_size,
                'child_chunk_size': config.child_chunk_size,
                'parent_overlap': config.parent_chunk_overlap,
                'child_overlap': config.child_chunk_overlap,
                'parent_separator': config.parent_separator,
                'child_separator': config.child_separator,
                'retrieval_mode': config.retrieval_mode,
                'top_k_children': config.top_k_children,
                'top_k_parents': config.top_k_parents,
                'enabled': config.enabled,
                **json.loads(config.config_json or '{}')
            }
        except:
            # è¿”å›é»˜è®¤é…ç½®
            return self.default_config.copy()
    
    def create_splitter(self, kb_id: str) -> SmartParentChildSplitter:
        """æ ¹æ®é…ç½®åˆ›å»ºåˆ†å‰²å™¨"""
        config = self.get_config(kb_id)
        
        return SmartParentChildSplitter(
            parent_chunk_size=config['parent_chunk_size'],
            child_chunk_size=config['child_chunk_size'],
            parent_overlap=config['parent_overlap'],
            child_overlap=config['child_overlap'],
            min_child_size=config.get('min_child_size', 10),
            parent_separator=config['parent_separator'],
            child_separator=config['child_separator']
        )