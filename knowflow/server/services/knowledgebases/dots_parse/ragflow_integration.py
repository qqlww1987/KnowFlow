#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DOTS OCR ä¸ RAGFlow é›†æˆæ¨¡å—

å°†DOTS OCRçš„è§£æç»“æœé›†æˆåˆ°RAGFlowçš„å­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿä¸­ï¼Œå¤ç”¨mineruçš„åˆ†å—å¤„ç†é€»è¾‘:
- ä½¿ç”¨å¢å¼ºçš„batch APIè¿›è¡Œåˆ†å—å­˜å‚¨
- å¤ç”¨mineruçš„ragflow_buildæ¨¡å—
- æ”¯æŒç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œå¤„ç†æµç¨‹
"""

import os
import json
import uuid
import logging
import tempfile
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path

# å¤ç”¨mineruçš„ç›¸å…³æ¨¡å—
from ..mineru_parse.ragflow_build import get_ragflow_doc, add_chunks_with_enhanced_batch_api
from ..mineru_parse.utils import update_document_progress
try:
    from database import get_minio_client
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›¸å¯¹å¯¼å…¥
    from ....database import get_minio_client

logger = logging.getLogger(__name__)

class RAGFlowIntegration:
    """DOTS OCR ä¸ RAGFlow çš„é›†æˆç±»ï¼Œå¤ç”¨mineruçš„å¤„ç†é€»è¾‘"""
    
    def __init__(self, kb_id: str, doc_id: str, embedding_config: Optional[Dict] = None):
        """åˆå§‹åŒ–RAGFlowé›†æˆ
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        """
        self.kb_id = kb_id
        self.doc_id = doc_id
        self.embedding_config = embedding_config or {}
        
        # æ•°æ®åº“å’Œå­˜å‚¨å®¢æˆ·ç«¯
        self.minio_client = get_minio_client()
        
        # è·å–RAGFlowæ–‡æ¡£å¯¹è±¡ï¼ˆå¤ç”¨minerué€»è¾‘ï¼‰
        self.doc, self.dataset = get_ragflow_doc(doc_id, kb_id)
    
    def create_chunks_in_ragflow(self, chunks: List[Dict[str, Any]], 
                                update_progress: Optional[Callable] = None) -> int:
        """å°†DOTSè§£æçš„chunkså­˜å‚¨åˆ°RAGFlowç³»ç»Ÿä¸­ï¼ˆå¤ç”¨mineruçš„batch APIï¼‰
        
        Args:
            chunks: DOTSå¤„ç†å™¨ç”Ÿæˆçš„åˆ†å—åˆ—è¡¨
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
            
        Returns:
            int: æˆåŠŸåˆ›å»ºçš„å—æ•°é‡
        """
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—éœ€è¦åˆ›å»º")
            return 0
        
        # è½¬æ¢DOTS chunksä¸ºRAGFlow batch APIæ ¼å¼
        batch_chunks = []
        for i, chunk_data in enumerate(chunks):
            content = chunk_data.get('content', '').strip()
            if not content:
                continue
                
            chunk_request = {
                "content": content,
                "important_keywords": [],  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…³é”®è¯æå–
                "questions": []  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é—®é¢˜ç”Ÿæˆ
            }
            
            # æ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆå®Œå…¨å¤ç”¨Mineruçš„é€»è¾‘å’Œæ ¼å¼ï¼‰
            # ç»Ÿä¸€æ’åºæœºåˆ¶ï¼šå›ºå®špage_num_int=1ï¼Œtop_int=åŸå§‹ç´¢å¼•ï¼ˆä¸Mineruä¿æŒä¸€è‡´ï¼‰
            chunk_request["page_num_int"] = [1]  # å›ºå®šä¸º1ï¼Œä¿è¯æ‰€æœ‰chunkséƒ½åœ¨åŒä¸€"é¡µ"
            chunk_request["top_int"] = i  # ä½¿ç”¨åˆ†å—ç´¢å¼•ä¿è¯é¡ºåº
            
            # å°è¯•è·å–ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼ˆä½œä¸ºé¢å¤–çš„ä½ç½®æ•°æ®ï¼Œä¸å½±å“æ’åºï¼‰
            if chunk_data.get('positions'):
                # ä½¿ç”¨ä»DOTSå…ƒç´ æ˜ å°„å¾—åˆ°çš„ç²¾ç¡®åæ ‡ï¼ˆMineruæ ¼å¼ï¼‰
                chunk_request["positions"] = chunk_data['positions']
                logger.debug(f"åˆ†å— {i}: æ‰¾åˆ°ç²¾ç¡®åæ ‡ ({len(chunk_data['positions'])} ä¸ªä½ç½®) + ç´¢å¼•æ’åº (page=1, top={i})")
            else:
                logger.debug(f"åˆ†å— {i}: ä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={i})")
            
            batch_chunks.append(chunk_request)
        
        if not batch_chunks:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„chunkséœ€è¦åˆ›å»º")
            return 0
        
        # ä½¿ç”¨mineruçš„batch APIï¼ˆå¤ç”¨ä»£ç ï¼‰
        chunk_contents = [chunk["content"] for chunk in batch_chunks]
        chunk_content_to_index = {content: i for i, content in enumerate(chunk_contents)}
        
        try:
            # ä½¿ç”¨DOTSä¸“ç”¨çš„batch APIè°ƒç”¨ï¼Œç›´æ¥ä¼ é€’åŒ…å«åæ ‡ä¿¡æ¯çš„batch_chunks
            chunk_count = self._add_dots_chunks_with_batch_api(
                batch_chunks, 
                update_progress
            )
            
            logger.info(f"æˆåŠŸåˆ›å»º {chunk_count} ä¸ªæ–‡æ¡£å—")
            return chunk_count
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨batch APIåˆ›å»ºchunkså¤±è´¥: {e}")
            raise
    
    def _add_dots_chunks_with_batch_api(self, batch_chunks: List[Dict[str, Any]], 
                                       update_progress: Optional[Callable] = None) -> int:
        """ä½¿ç”¨batch APIæ·»åŠ DOTS chunksï¼Œä¿æŒåæ ‡ä¿¡æ¯
        
        Args:
            batch_chunks: åŒ…å«åæ ‡ä¿¡æ¯çš„batchæ•°æ®
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
            
        Returns:
            int: æˆåŠŸæ·»åŠ çš„åˆ†å—æ•°é‡
        """
        if not batch_chunks:
            if update_progress:
                update_progress(0.8, "æ²¡æœ‰chunkséœ€è¦æ·»åŠ ")
            return 0
        
        if update_progress:
            update_progress(0.8, f"å¼€å§‹æ‰¹é‡æ·»åŠ {len(batch_chunks)}ä¸ªDOTS chunks...")
        
        try:
            import requests
            import json
            
            # è·å–APIåŸºæœ¬ä¿¡æ¯ï¼ˆå¤ç”¨mineruçš„æ–¹å¼ï¼‰
            base_url = self.doc.rag.api_url
            headers = self.doc.rag.authorization_header
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "chunks": batch_chunks,
                "batch_size": 20
            }
            
            # è°ƒç”¨å¢å¼ºçš„batchæ¥å£
            api_url = f"{base_url}/datasets/{self.doc.dataset_id}/documents/{self.doc.id}/chunks/batch"
            logger.info(f"ğŸ”— å‘é€DOTS batchè¯·æ±‚åˆ°: {api_url}")
            logger.debug(f"ğŸ“¦ å‘é€ {len(batch_chunks)} ä¸ªchunksï¼Œå…¶ä¸­ {sum(1 for c in batch_chunks if c.get('positions'))} ä¸ªæœ‰åæ ‡ä¿¡æ¯")
            
            response = requests.post(api_url, json=request_data, headers=headers)
            
            logger.info(f"ğŸ“¥ DOTS batchæ¥å£å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    if result.get("code") == 0:
                        # æ‰¹é‡æ·»åŠ æˆåŠŸ
                        data = result.get("data", {})
                        added = data.get("total_added", 0)
                        failed = data.get("total_failed", 0)
                        
                        logger.info(f"âœ… DOTS batchæ¥å£å¤„ç†å®Œæˆ: æˆåŠŸ {added} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª")
                        
                        # ç»Ÿè®¡åæ ‡ä¿¡æ¯
                        coords_count = sum(1 for chunk in batch_chunks if chunk.get('positions'))
                        logger.info(f"ğŸ“ åŒ…å«åæ ‡ä¿¡æ¯çš„åˆ†å—: {coords_count}/{len(batch_chunks)}")
                        
                        if update_progress:
                            update_progress(0.95, f"DOTS batchå¤„ç†å®Œæˆ: æˆåŠŸ {added}/{len(batch_chunks)} chunks")
                        return added
                    else:
                        # æ‰¹é‡æ·»åŠ å¤±è´¥
                        error_msg = result.get("message", "Unknown error")
                        logger.error(f"âŒ DOTS batchæ¥å£å¤±è´¥: {error_msg}")
                        if update_progress:
                            update_progress(0.95, f"DOTS batchå¤„ç†å¤±è´¥: {error_msg}")
                        return 0
                except json.JSONDecodeError:
                    logger.error(f"âŒ DOTS batchæ¥å£å“åº”è§£æå¤±è´¥")
                    if update_progress:
                        update_progress(0.95, "å“åº”è§£æå¤±è´¥")
                    return 0
            else:
                logger.error(f"âŒ DOTS batchæ¥å£HTTPé”™è¯¯: {response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                if update_progress:
                    update_progress(0.95, f"HTTPé”™è¯¯: {response.status_code}")
                return 0
                
        except Exception as e:
            if update_progress:
                update_progress(0.95, f"DOTS batchå¤„ç†å¼‚å¸¸: {str(e)}")
            logger.error(f"âŒ DOTS batchå¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def save_markdown_to_minio(self, markdown_content: str, 
                              bucket_name: Optional[str] = None) -> Optional[str]:
        """å°†Markdownå†…å®¹ä¿å­˜åˆ°MinIO
        
        Args:
            markdown_content: Markdownå†…å®¹
            bucket_name: å­˜å‚¨æ¡¶åç§°ï¼Œé»˜è®¤ä½¿ç”¨çŸ¥è¯†åº“ID
            
        Returns:
            str: MinIOä¸­çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not markdown_content:
            logger.warning("Markdownå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return None
        
        try:
            bucket = bucket_name or self.kb_id
            
            # ç¡®ä¿bucketå­˜åœ¨
            if not self.minio_client.bucket_exists(bucket):
                logger.warning(f"MinIO bucket {bucket} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                return None
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_name = f"{self.doc_id}_dots_output.md"
            file_path = f"dots_output/{file_name}"
            
            # ä¸Šä¼ å†…å®¹
            from io import BytesIO
            content_bytes = markdown_content.encode('utf-8')
            content_stream = BytesIO(content_bytes)
            
            self.minio_client.put_object(
                bucket,
                file_path,
                content_stream,
                length=len(content_bytes),
                content_type='text/markdown'
            )
            
            logger.info(f"Markdownå†…å®¹å·²ä¿å­˜åˆ° MinIO: {bucket}/{file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜Markdownåˆ°MinIOå¤±è´¥: {e}")
            return None
    
    def create_elasticsearch_entries(self, chunks: List[Dict[str, Any]]) -> bool:
        """åˆ›å»ºElasticsearchç´¢å¼•æ¡ç›®ï¼ˆç”±batch APIè‡ªåŠ¨å¤„ç†ï¼‰
        
        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        # batch APIä¼šè‡ªåŠ¨å¤„ç†Elasticsearchç´¢å¼•ï¼Œæ— éœ€å•ç‹¬å¤„ç†
        logger.info("Elasticsearchç´¢å¼•ç”±batch APIè‡ªåŠ¨å¤„ç†")
        return True
    
    def process_and_store(self, processor_result: Dict[str, Any], 
                         update_progress: Optional[Callable] = None) -> Dict[str, Any]:
        """å¤„ç†DOTSç»“æœå¹¶å­˜å‚¨åˆ°RAGFlowï¼ˆå¤ç”¨mineruçš„å¤„ç†é€»è¾‘ï¼‰
        
        Args:
            processor_result: DOTSå¤„ç†å™¨çš„ç»“æœ
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
            
        Returns:
            dict: å­˜å‚¨ç»“æœ
        """
        try:
            if not processor_result.get('success', False):
                return {
                    'success': False,
                    'error': 'DOTSå¤„ç†å™¨ç»“æœæ— æ•ˆ',
                    'chunk_count': 0
                }
            
            chunks = processor_result.get('chunks', [])
            markdown_content = processor_result.get('markdown_content', '')
            
            if update_progress:
                update_progress(0.4, "å¼€å§‹ä¿å­˜è§£æç»“æœ")
            
            # 1. ä½¿ç”¨batch APIåˆ›å»ºæ–‡æ¡£å—ï¼ˆå¤ç”¨minerué€»è¾‘ï¼‰
            chunk_count = self.create_chunks_in_ragflow(chunks, update_progress)
            
            if update_progress:
                update_progress(0.8, "ä¿å­˜Markdownåˆ°å­˜å‚¨")
            
            # 2. ä¿å­˜Markdownåˆ°MinIOï¼ˆå¯é€‰ï¼‰
            markdown_path = self.save_markdown_to_minio(markdown_content)
            
            # 3. Elasticsearchç´¢å¼•ç”±batch APIè‡ªåŠ¨å¤„ç†
            es_success = self.create_elasticsearch_entries(chunks)
            
            # 4. æ›´æ–°æ–‡æ¡£è¿›åº¦ï¼ˆå¤ç”¨mineruçš„é€»è¾‘ï¼‰
            if update_progress:
                update_progress(0.95, f"å®Œæˆæ•°æ®å­˜å‚¨ï¼ŒæˆåŠŸå¤„ç† {chunk_count} ä¸ªchunks")
            
            # ä½¿ç”¨mineruçš„è¿›åº¦æ›´æ–°å‡½æ•°
            update_document_progress(
                self.doc_id, 
                progress=1.0, 
                message=f"DOTSå¤„ç†å®Œæˆï¼Œå…± {chunk_count} ä¸ªchunks",
                chunk_count=chunk_count
            )
            
            return {
                'success': True,
                'chunk_count': chunk_count,
                'markdown_saved': markdown_path is not None,
                'markdown_path': markdown_path,
                'elasticsearch_indexed': es_success,
                'elements_count': processor_result.get('elements_count', 0),
                'pages_count': processor_result.get('pages_count', 0)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†å’Œå­˜å‚¨DOTSç»“æœå¤±è´¥: {e}")
            # æ›´æ–°é”™è¯¯çŠ¶æ€
            update_document_progress(
                self.doc_id, 
                progress=1.0, 
                message=f"DOTSå¤„ç†å¤±è´¥: {str(e)}",
                status="0"  # æ ‡è®°ä¸ºå¤±è´¥
            )
            return {
                'success': False,
                'error': f'å­˜å‚¨å¤±è´¥: {str(e)}',
                'chunk_count': 0
            }

def create_ragflow_resources(doc_id: str, kb_id: str, 
                           processor_result: Dict[str, Any],
                           update_progress: Optional[Callable] = None,
                           embedding_config: Optional[Dict] = None) -> int:
    """åˆ›å»ºRAGFlowèµ„æºçš„ä¾¿æ·å‡½æ•°ï¼ˆå¤ç”¨mineruè®¾è®¡æ¨¡å¼ï¼‰
    
    Args:
        doc_id: æ–‡æ¡£ID
        kb_id: çŸ¥è¯†åº“ID
        processor_result: DOTSå¤„ç†å™¨ç»“æœ
        update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
        embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        
    Returns:
        int: åˆ›å»ºçš„æ–‡æ¡£å—æ•°é‡
    """
    try:
        integration = RAGFlowIntegration(kb_id, doc_id, embedding_config)
        result = integration.process_and_store(processor_result, update_progress)
        
        if result['success']:
            logger.info(f"DOTS RAGFlowèµ„æºåˆ›å»ºæˆåŠŸ: {result['chunk_count']} ä¸ªå—")
            return result['chunk_count']
        else:
            logger.error(f"DOTS RAGFlowèµ„æºåˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
            return 0
            
    except Exception as e:
        logger.error(f"åˆ›å»ºDOTS RAGFlowèµ„æºå¼‚å¸¸: {e}")
        # ç¡®ä¿é”™è¯¯çŠ¶æ€è¢«è®°å½•
        try:
            update_document_progress(
                doc_id, 
                progress=1.0, 
                message=f"DOTSå¤„ç†å¼‚å¸¸: {str(e)}",
                status="0"
            )
        except:
            pass
        return 0