#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DOTS OCR ä¸ RAGFlow é›†æˆæ¨¡å—

å°†DOTS OCRçš„è§£æç»“æœé›†æˆåˆ°RAGFlowçš„å­˜å‚¨å’Œæ£€ç´¢ç³»ç»Ÿä¸­ï¼Œå¤ç”¨mineruçš„åˆ†å—å¤„ç†é€»è¾‘:
- ä½¿ç”¨å¢å¼ºçš„batch APIè¿›è¡Œåˆ†å—å­˜å‚¨
- å¤ç”¨mineruçš„ragflow_buildæ¨¡å—
- æ”¯æŒç»Ÿä¸€çš„æ•°æ®æ ¼å¼å’Œå¤„ç†æµç¨‹
"""

import os
import json
import uuid
import logging
import tempfile
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path

# å¤ç”¨mineruçš„ç›¸å…³æ¨¡å—
from ..mineru_parse.ragflow_build import get_ragflow_doc, add_chunks_with_enhanced_batch_api
from ..mineru_parse.utils import update_document_progress
try:
    from database import get_minio_client
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›¸å¯¹å¯¼å…¥
    from ....database import get_minio_client

logger = logging.getLogger(__name__)

class RAGFlowIntegration:
    """DOTS OCR ä¸ RAGFlow çš„é›†æˆç±»ï¼Œå¤ç”¨mineruçš„å¤„ç†é€»è¾‘"""
    
    def __init__(self, kb_id: str, doc_id: str, embedding_config: Optional[Dict] = None):
        """åˆå§‹åŒ–RAGFlowé›†æˆ
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        """
        self.kb_id = kb_id
        self.doc_id = doc_id
        self.embedding_config = embedding_config or {}
        
        # æ•°æ®åº“å’Œå­˜å‚¨å®¢æˆ·ç«¯
        self.minio_client = get_minio_client()
        
        # è·å–RAGFlowæ–‡æ¡£å¯¹è±¡ï¼ˆå¤ç”¨minerué€»è¾‘ï¼‰
        self.doc, self.dataset = get_ragflow_doc(doc_id, kb_id)
    
    def create_chunks_in_ragflow_unified(self, processor_result: Dict[str, Any], 
                                        update_progress: Optional[Callable] = None) -> int:
        """ç»Ÿä¸€çš„RAGFlowé›†æˆ - å®Œå…¨å¤ç”¨MinerUçš„batch APIå’Œçˆ¶å­åˆ†å—å¤„ç†
        
        Args:
            processor_result: DOTSç»Ÿä¸€å¤„ç†å™¨çš„ç»“æœ
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
            
        Returns:
            int: æˆåŠŸåˆ›å»ºçš„å—æ•°é‡
        """
        chunks = processor_result.get('chunks', [])
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—éœ€è¦åˆ›å»º")
            return 0
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºçˆ¶å­åˆ†å—
        is_parent_child = processor_result.get('is_parent_child', False)
        
        if is_parent_child:
            # çˆ¶å­åˆ†å—æµç¨‹ - å¤ç”¨MinerUçš„parent_child_dataæ ¼å¼
            return self._handle_parent_child_chunks(processor_result, update_progress)
        else:
            # æ™®é€šåˆ†å—æµç¨‹ - å¤ç”¨MinerUçš„æ ‡å‡†batch API
            return self._handle_standard_chunks(processor_result, update_progress)
    
    def _handle_parent_child_chunks(self, processor_result: Dict, update_progress: Callable) -> int:
        """å¤„ç†çˆ¶å­åˆ†å— - å®Œå…¨å¤ç”¨MinerUé€»è¾‘"""
        
        # æ„é€ MinerUæ ¼å¼çš„parent_child_data
        parent_child_data = {
            'doc_id': self.doc_id,
            'kb_id': self.kb_id,
            'parent_chunks': processor_result.get('parent_chunks', []),
            'child_chunks': processor_result.get('child_chunks', []),
            'relationships': processor_result.get('relationships', [])
        }
        
        # æå–å­åˆ†å—å†…å®¹å’Œåæ ‡ä¿¡æ¯ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        child_chunks_data = parent_child_data['child_chunks']
        chunks_content = []
        chunks_with_positions = []  # ä¿å­˜å¸¦åæ ‡ä¿¡æ¯çš„åˆ†å—
        
        for i, chunk_info in enumerate(child_chunks_data):
            if hasattr(chunk_info, 'content'):
                # å¦‚æœæ˜¯ChunkInfoå¯¹è±¡
                content = chunk_info.content
            elif isinstance(chunk_info, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                content = chunk_info.get('content', '')
            else:
                content = str(chunk_info)
            
            chunks_content.append(content)
            
            # æ„é€ å¸¦åæ ‡ä¿¡æ¯çš„åˆ†å—æ•°æ®
            chunk_with_coords = {
                "content": content,
                "important_keywords": [],
                "questions": []
            }
            
            # æ·»åŠ ä½ç½®ä¿¡æ¯
            chunk_with_coords["page_num_int"] = [1]  # å›ºå®šä¸º1çš„æ’åº
            chunk_with_coords["top_int"] = i  # ä½¿ç”¨ç´¢å¼•æ’åº
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®åæ ‡ä¿¡æ¯
            if isinstance(chunk_info, dict) and chunk_info.get('positions'):
                chunk_with_coords["positions"] = chunk_info['positions']
                logger.debug(f"çˆ¶å­åˆ†å— {i}: æ‰¾åˆ°ç²¾ç¡®åæ ‡ ({len(chunk_info['positions'])} ä¸ªä½ç½®)")
            elif isinstance(chunk_info, dict) and chunk_info.get('has_coordinates'):
                # å¦‚æœæœ‰has_coordinatesæ ‡è®°ä½†æ²¡æœ‰positionsï¼Œè®°å½•è­¦å‘Š
                logger.warning(f"çˆ¶å­åˆ†å— {i}: has_coordinates=True ä½†æ‰¾ä¸åˆ°positions")
            
            chunks_with_positions.append(chunk_with_coords)
        
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks_content)}
        
        coords_count = sum(1 for c in chunks_with_positions if 'positions' in c)
        logger.info(f"DOTSçˆ¶å­åˆ†å—åæ ‡æ£€æŸ¥: {coords_count}/{len(chunks_with_positions)} ä¸ªå­å—æœ‰ç²¾ç¡®åæ ‡")
        
        logger.info(f"DOTSçˆ¶å­åˆ†å—å¤„ç†: {len(parent_child_data.get('parent_chunks', []))}çˆ¶å—, "
                   f"{len(child_chunks_data)}å­å—, {len(parent_child_data.get('relationships', []))}æ˜ å°„å…³ç³»")
        
        # ç›´æ¥è°ƒç”¨MinerUçš„å¢å¼ºbatch API
        from ..mineru_parse.ragflow_build import add_chunks_with_enhanced_batch_api
        
        # ä¼ é€’å¸¦åæ ‡ä¿¡æ¯çš„åˆ†å—æ•°æ®ç»™batch API
        return self._call_enhanced_batch_api_with_coordinates(
            chunks_with_positions,
            parent_child_data,
            update_progress
        )
    
    def _call_enhanced_batch_api_with_coordinates(self, chunks_with_positions: List[Dict], 
                                                 parent_child_data: Dict, 
                                                 update_progress: Callable) -> int:
        """è°ƒç”¨å¢å¼ºbatch APIï¼Œç¡®ä¿åæ ‡ä¿¡æ¯è¢«æ­£ç¡®ä¼ é€’"""
        try:
            # å¯¼å…¥MinerUçš„å¢å¼ºbatch API
            from ..mineru_parse.ragflow_build import add_chunks_with_enhanced_batch_api
            
            # å‡†å¤‡å­åˆ†å—å†…å®¹å’Œç´¢å¼•æ˜ å°„
            child_chunks_content = []
            chunk_content_to_index = {}
            chunks_with_coords = []
            
            for i, chunk_info in enumerate(chunks_with_positions):
                content = chunk_info.get('content', '').strip()
                if content:
                    child_chunks_content.append(content)
                    chunk_content_to_index[content] = i
                    
                    # æ„å»ºåŒ…å«åæ ‡çš„åˆ†å—æ•°æ®
                    chunk_with_coords = {
                        'content': content,
                        'index': i
                    }
                    
                    # å¦‚æœæœ‰åæ ‡ä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆ†å—æ•°æ®ä¸­
                    if chunk_info.get('positions'):
                        chunk_with_coords['positions'] = chunk_info['positions']
                        logger.debug(f"å­åˆ†å—{i}åŒ…å«åæ ‡: {len(chunk_info['positions'])}ä¸ªä½ç½®")
                    
                    chunks_with_coords.append(chunk_with_coords)
            
            logger.info(f"è°ƒç”¨å¢å¼ºbatch API: {len(child_chunks_content)}ä¸ªå­åˆ†å—å†…å®¹ï¼Œ"
                       f"{len(chunks_with_coords)}ä¸ªåŒ…å«åæ ‡ä¿¡æ¯çš„åˆ†å—")
            
            # åœ¨parent_child_dataä¸­æ·»åŠ åæ ‡ä¿¡æ¯
            enhanced_parent_child_data = parent_child_data.copy()
            enhanced_parent_child_data['chunks_with_coords'] = chunks_with_coords
            
            # è°ƒç”¨MinerUçš„å¢å¼ºbatch APIï¼Œä¼ é€’å®Œæ•´çš„åæ ‡ä¿¡æ¯
            return add_chunks_with_enhanced_batch_api(
                doc=self.doc,
                chunks=child_chunks_content,
                md_file_path=None,  # DOTSä¸éœ€è¦mdæ–‡ä»¶è·¯å¾„
                chunk_content_to_index=chunk_content_to_index,
                update_progress=update_progress,
                parent_child_data=enhanced_parent_child_data,  # ä¼ é€’å¢å¼ºçš„çˆ¶å­æ•°æ®ï¼ˆåŒ…å«åæ ‡ï¼‰
                chunks_with_coordinates=chunks_with_coords  # ä¼ é€’DOTSåæ ‡ä¿¡æ¯
            )
            
        except Exception as e:
            logger.error(f"è°ƒç”¨å¢å¼ºbatch APIå¤±è´¥: {e}")
            raise
    
    def _handle_standard_chunks(self, processor_result: Dict, update_progress: Callable) -> int:
        """å¤„ç†æ ‡å‡†åˆ†å— - å¤ç”¨MinerUçš„batch APIï¼ŒåŒ…å«åæ ‡ä¿¡æ¯"""
        
        chunks = processor_result.get('chunks', [])
        
        # æå–åˆ†å—å†…å®¹å’Œåæ ‡ä¿¡æ¯
        chunks_content = []
        chunks_with_coordinates = []
        
        for chunk in chunks:
            content = chunk.get('content', '').strip()
            if content:
                chunks_content.append(content)
                
                # æ„å»ºåŒ…å«åæ ‡ä¿¡æ¯çš„åˆ†å—æ•°æ®
                chunk_with_coord = {
                    'content': content
                }
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åæ ‡ä¿¡æ¯
                if chunk.get('positions'):
                    chunk_with_coord['positions'] = chunk['positions']
                    logger.debug(f"æ ‡å‡†åˆ†å—åŒ…å«åæ ‡: {len(chunk['positions'])}ä¸ªä½ç½®")
                
                chunks_with_coordinates.append(chunk_with_coord)
        
        if not chunks_content:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ ‡å‡†åˆ†å—å†…å®¹")
            return 0
        
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks_content)}
        coords_count = sum(1 for c in chunks_with_coordinates if c.get('positions'))
        
        logger.info(f"DOTSæ ‡å‡†åˆ†å—å¤„ç†: {len(chunks_content)}ä¸ªåˆ†å—ï¼Œå…¶ä¸­{coords_count}ä¸ªæœ‰åæ ‡ä¿¡æ¯")
        
        # è°ƒç”¨MinerUçš„å¢å¼ºbatch APIï¼Œä¼ é€’åæ ‡ä¿¡æ¯
        from ..mineru_parse.ragflow_build import add_chunks_with_enhanced_batch_api
        
        return add_chunks_with_enhanced_batch_api(
            doc=self.doc,
            chunks=chunks_content,
            md_file_path=None,  # DOTSä¸éœ€è¦mdæ–‡ä»¶
            chunk_content_to_index=chunk_content_to_index,
            update_progress=update_progress,
            parent_child_data=None,  # æ ‡å‡†åˆ†å—ä¸ä¼ é€’çˆ¶å­æ•°æ®
            chunks_with_coordinates=chunks_with_coordinates  # ä¼ é€’DOTSåæ ‡ä¿¡æ¯
        )
    
    def create_chunks_in_ragflow(self, chunks: List[Dict[str, Any]], 
                                update_progress: Optional[Callable] = None) -> int:
        """åŸæœ‰æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰â€”â€”å°†DOTSè§£æçš„chunkså­˜å‚¨åˆ°RAGFlowç³»ç»Ÿä¸­ï¼ˆå¤ç”¨mineruçš„batch APIï¼‰
        
        Args:
            chunks: DOTSå¤„ç†å™¨ç”Ÿæˆçš„åˆ†å—åˆ—è¡¨
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
            
        Returns:
            int: æˆåŠŸåˆ›å»ºçš„å—æ•°é‡
        """
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—éœ€è¦åˆ›å»º")
            return 0
        
        # è½¬æ¢DOTS chunksä¸ºRAGFlow batch APIæ ¼å¼
        batch_chunks = []
        for i, chunk_data in enumerate(chunks):
            content = chunk_data.get('content', '').strip()
            if not content:
                continue
                
            chunk_request = {
                "content": content,
                "important_keywords": [],  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…³é”®è¯æå–
                "questions": []  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é—®é¢˜ç”Ÿæˆ
            }
            
            # æ·»åŠ ä½ç½®ä¿¡æ¯ï¼ˆå®Œå…¨å¤ç”¨Mineruçš„é€»è¾‘å’Œæ ¼å¼ï¼‰
            # ç»Ÿä¸€æ’åºæœºåˆ¶ï¼šå›ºå®špage_num_int=1ï¼Œtop_int=åŸå§‹ç´¢å¼•ï¼ˆä¸Mineruä¿æŒä¸€è‡´ï¼‰
            chunk_request["page_num_int"] = [1]  # å›ºå®šä¸º1ï¼Œä¿è¯æ‰€æœ‰chunkséƒ½åœ¨åŒä¸€"é¡µ"
            chunk_request["top_int"] = i  # ä½¿ç”¨åˆ†å—ç´¢å¼•ä¿è¯é¡ºåº
            
            # å°è¯•è·å–ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼ˆä½œä¸ºé¢å¤–çš„ä½ç½®æ•°æ®ï¼Œä¸å½±å“æ’åºï¼‰
            if chunk_data.get('positions'):
                # ä½¿ç”¨ä»DOTSå…ƒç´ æ˜ å°„å¾—åˆ°çš„ç²¾ç¡®åæ ‡ï¼ˆMineruæ ¼å¼ï¼‰
                chunk_request["positions"] = chunk_data['positions']
                logger.debug(f"åˆ†å— {i}: æ‰¾åˆ°ç²¾ç¡®åæ ‡ ({len(chunk_data['positions'])} ä¸ªä½ç½®) + ç´¢å¼•æ’åº (page=1, top={i})")
            else:
                logger.debug(f"åˆ†å— {i}: ä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={i})")
            
            batch_chunks.append(chunk_request)
        
        if not batch_chunks:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„chunkséœ€è¦åˆ›å»º")
            return 0
        
        # ä½¿ç”¨mineruçš„batch APIï¼ˆå¤ç”¨ä»£ç ï¼‰
        chunk_contents = [chunk["content"] for chunk in batch_chunks]
        chunk_content_to_index = {content: i for i, content in enumerate(chunk_contents)}
        
        try:
            # ä½¿ç”¨DOTSä¸“ç”¨çš„batch APIè°ƒç”¨ï¼Œç›´æ¥ä¼ é€’åŒ…å«åæ ‡ä¿¡æ¯çš„batch_chunks
            chunk_count = self._add_dots_chunks_with_batch_api(
                batch_chunks, 
                update_progress
            )
            
            logger.info(f"æˆåŠŸåˆ›å»º {chunk_count} ä¸ªæ–‡æ¡£å—")
            return chunk_count
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨batch APIåˆ›å»ºchunkså¤±è´¥: {e}")
            raise
    
    def _add_dots_chunks_with_batch_api(self, batch_chunks: List[Dict[str, Any]], 
                                       update_progress: Optional[Callable] = None) -> int:
        """ä½¿ç”¨batch APIæ·»åŠ DOTS chunksï¼Œä¿æŒåæ ‡ä¿¡æ¯
        
        Args:
            batch_chunks: åŒ…å«åæ ‡ä¿¡æ¯çš„batchæ•°æ®
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
            
        Returns:
            int: æˆåŠŸæ·»åŠ çš„åˆ†å—æ•°é‡
        """
        if not batch_chunks:
            if update_progress:
                update_progress(0.8, "æ²¡æœ‰chunkséœ€è¦æ·»åŠ ")
            return 0
        
        if update_progress:
            update_progress(0.8, f"å¼€å§‹æ‰¹é‡æ·»åŠ {len(batch_chunks)}ä¸ªDOTS chunks...")
        
        try:
            import requests
            import json
            
            # è·å–APIåŸºæœ¬ä¿¡æ¯ï¼ˆå¤ç”¨mineruçš„æ–¹å¼ï¼‰
            base_url = self.doc.rag.api_url
            headers = self.doc.rag.authorization_header
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "chunks": batch_chunks,
                "batch_size": 20
            }
            
            # è°ƒç”¨å¢å¼ºçš„batchæ¥å£
            api_url = f"{base_url}/datasets/{self.doc.dataset_id}/documents/{self.doc.id}/chunks/batch"
            logger.info(f"ğŸ”— å‘é€DOTS batchè¯·æ±‚åˆ°: {api_url}")
            logger.debug(f"ğŸ“¦ å‘é€ {len(batch_chunks)} ä¸ªchunksï¼Œå…¶ä¸­ {sum(1 for c in batch_chunks if c.get('positions'))} ä¸ªæœ‰åæ ‡ä¿¡æ¯")
            
            response = requests.post(api_url, json=request_data, headers=headers)
            
            logger.info(f"ğŸ“¥ DOTS batchæ¥å£å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    if result.get("code") == 0:
                        # æ‰¹é‡æ·»åŠ æˆåŠŸ
                        data = result.get("data", {})
                        added = data.get("total_added", 0)
                        failed = data.get("total_failed", 0)
                        
                        logger.info(f"âœ… DOTS batchæ¥å£å¤„ç†å®Œæˆ: æˆåŠŸ {added} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª")
                        
                        # ç»Ÿè®¡åæ ‡ä¿¡æ¯
                        coords_count = sum(1 for chunk in batch_chunks if chunk.get('positions'))
                        logger.info(f"ğŸ“ åŒ…å«åæ ‡ä¿¡æ¯çš„åˆ†å—: {coords_count}/{len(batch_chunks)}")
                        
                        if update_progress:
                            update_progress(0.95, f"DOTS batchå¤„ç†å®Œæˆ: æˆåŠŸ {added}/{len(batch_chunks)} chunks")
                        return added
                    else:
                        # æ‰¹é‡æ·»åŠ å¤±è´¥
                        error_msg = result.get("message", "Unknown error")
                        logger.error(f"âŒ DOTS batchæ¥å£å¤±è´¥: {error_msg}")
                        if update_progress:
                            update_progress(0.95, f"DOTS batchå¤„ç†å¤±è´¥: {error_msg}")
                        return 0
                except json.JSONDecodeError:
                    logger.error(f"âŒ DOTS batchæ¥å£å“åº”è§£æå¤±è´¥")
                    if update_progress:
                        update_progress(0.95, "å“åº”è§£æå¤±è´¥")
                    return 0
            else:
                logger.error(f"âŒ DOTS batchæ¥å£HTTPé”™è¯¯: {response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                if update_progress:
                    update_progress(0.95, f"HTTPé”™è¯¯: {response.status_code}")
                return 0
                
        except Exception as e:
            if update_progress:
                update_progress(0.95, f"DOTS batchå¤„ç†å¼‚å¸¸: {str(e)}")
            logger.error(f"âŒ DOTS batchå¤„ç†å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def save_markdown_to_minio(self, markdown_content: str, 
                              bucket_name: Optional[str] = None) -> Optional[str]:
        """å°†Markdownå†…å®¹ä¿å­˜åˆ°MinIO
        
        Args:
            markdown_content: Markdownå†…å®¹
            bucket_name: å­˜å‚¨æ¡¶åç§°ï¼Œé»˜è®¤ä½¿ç”¨çŸ¥è¯†åº“ID
            
        Returns:
            str: MinIOä¸­çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not markdown_content:
            logger.warning("Markdownå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return None
        
        try:
            bucket = bucket_name or self.kb_id
            
            # ç¡®ä¿bucketå­˜åœ¨
            if not self.minio_client.bucket_exists(bucket):
                logger.warning(f"MinIO bucket {bucket} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                return None
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_name = f"{self.doc_id}_dots_output.md"
            file_path = f"dots_output/{file_name}"
            
            # ä¸Šä¼ å†…å®¹
            from io import BytesIO
            content_bytes = markdown_content.encode('utf-8')
            content_stream = BytesIO(content_bytes)
            
            self.minio_client.put_object(
                bucket,
                file_path,
                content_stream,
                length=len(content_bytes),
                content_type='text/markdown'
            )
            
            logger.info(f"Markdownå†…å®¹å·²ä¿å­˜åˆ° MinIO: {bucket}/{file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜Markdownåˆ°MinIOå¤±è´¥: {e}")
            return None
    
    def create_elasticsearch_entries(self, chunks: List[Dict[str, Any]]) -> bool:
        """åˆ›å»ºElasticsearchç´¢å¼•æ¡ç›®ï¼ˆç”±batch APIè‡ªåŠ¨å¤„ç†ï¼‰
        
        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        # batch APIä¼šè‡ªåŠ¨å¤„ç†Elasticsearchç´¢å¼•ï¼Œæ— éœ€å•ç‹¬å¤„ç†
        logger.info("Elasticsearchç´¢å¼•ç”±batch APIè‡ªåŠ¨å¤„ç†")
        return True
    
    def process_and_store(self, processor_result: Dict[str, Any], 
                         update_progress: Optional[Callable] = None) -> Dict[str, Any]:
        """å¤„ç†DOTSç»“æœå¹¶å­˜å‚¨åˆ°RAGFlowï¼ˆä½¿ç”¨ç»Ÿä¸€åˆ†å—æ¥å£ï¼‰
        
        Args:
            processor_result: DOTSç»Ÿä¸€å¤„ç†å™¨çš„ç»“æœ
            update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
            
        Returns:
            dict: å­˜å‚¨ç»“æœ
        """
        try:
            if not processor_result.get('success', False):
                return {
                    'success': False,
                    'error': 'DOTSå¤„ç†å™¨ç»“æœæ— æ•ˆ',
                    'chunk_count': 0
                }
            
            chunks = processor_result.get('chunks', [])
            markdown_content = processor_result.get('markdown_content', '')
            is_parent_child = processor_result.get('is_parent_child', False)
            
            if update_progress:
                progress_msg = f"å¼€å§‹ä¿å­˜DOTSè§£æç»“æœ ({'Parent-Child' if is_parent_child else 'Standard'} æ¨¡å¼)"
                update_progress(0.4, progress_msg)
            
            # 1. ä½¿ç”¨ç»Ÿä¸€çš„batch APIåˆ›å»ºæ–‡æ¡£å—ï¼ˆå®Œå…¨å¤ç”¨MinerUé€»è¾‘ï¼‰
            chunk_count = self.create_chunks_in_ragflow_unified(processor_result, update_progress)
            
            # 2. ä¿å­˜Markdownåˆ°MinIOï¼ˆå¯é€‰ï¼‰
            if update_progress:
                update_progress(0.8, "ä¿å­˜Markdownåˆ°å­˜å‚¨")
            
            markdown_path = self.save_markdown_to_minio(markdown_content)
            
            # 3. Elasticsearchç´¢å¼•ç”±batch APIè‡ªåŠ¨å¤„ç†
            es_success = self.create_elasticsearch_entries(chunks)
            
            # 4. æ›´æ–°æ–‡æ¡£è¿›åº¦ï¼ˆå¤ç”¨MinerUçš„é€»è¾‘ï¼‰
            progress_msg = f"å®ŒæˆDOTSæ•°æ®å­˜å‚¨ï¼ŒæˆåŠŸå¤„ç† {chunk_count} ä¸ªchunks"
            if is_parent_child:
                total_parents = processor_result.get('total_parents', 0)
                total_children = processor_result.get('total_children', 0)
                progress_msg += f" (çˆ¶å—:{total_parents}, å­å—:{total_children})"
            
            if update_progress:
                update_progress(0.95, progress_msg)
            
            # ä½¿ç”¨MinerUçš„è¿›åº¦æ›´æ–°å‡½æ•°
            update_document_progress(
                self.doc_id, 
                progress=1.0, 
                message=f"DOTSç»Ÿä¸€å¤„ç†å®Œæˆï¼Œå…± {chunk_count} ä¸ªchunks" + 
                        (f" (çˆ¶å­åˆ†å—)" if is_parent_child else ""),
                chunk_count=chunk_count
            )
            
            # æ„å»ºè¿”å›ç»“æœï¼ŒåŒ…å«ç»Ÿä¸€åˆ†å—çš„æ‰€æœ‰ä¿¡æ¯
            result = {
                'success': True,
                'chunk_count': chunk_count,
                'markdown_saved': markdown_path is not None,
                'markdown_path': markdown_path,
                'elasticsearch_indexed': es_success,
                'elements_count': processor_result.get('elements_count', 0),
                'pages_count': processor_result.get('pages_count', 0),
                'chunking_strategy': processor_result.get('chunking_strategy', 'unknown'),
                'coordinate_source': processor_result.get('coordinate_source', 'dots'),
                'has_coordinates': processor_result.get('has_coordinates', False)
            }
            
            # å¦‚æœæ˜¯çˆ¶å­åˆ†å—ï¼Œæ·»åŠ çˆ¶å­åˆ†å—ç»“æœ
            if is_parent_child:
                result.update({
                    'is_parent_child': True,
                    'total_parents': processor_result.get('total_parents', 0),
                    'total_children': processor_result.get('total_children', 0),
                    'parent_child_relationships': len(processor_result.get('relationships', []))
                })
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†å’Œå­˜å‚¨DOTSç»“æœå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # æ›´æ–°é”™è¯¯çŠ¶æ€
            update_document_progress(
                self.doc_id, 
                progress=1.0, 
                message=f"DOTSå¤„ç†å¤±è´¥: {str(e)}",
                status="0"  # æ ‡è®°ä¸ºå¤±è´¥
            )
            return {
                'success': False,
                'error': f'å­˜å‚¨å¤±è´¥: {str(e)}',
                'chunk_count': 0,
                'is_parent_child': processor_result.get('is_parent_child', False)
            }

def create_ragflow_resources(doc_id: str, kb_id: str, 
                           processor_result: Dict[str, Any],
                           update_progress: Optional[Callable] = None,
                           embedding_config: Optional[Dict] = None) -> int:
    """åˆ›å»ºRAGFlowèµ„æºçš„ä¾¿æ·å‡½æ•°ï¼ˆä½¿ç”¨ç»Ÿä¸€RAGFlowé›†æˆï¼Œå®Œå…¨å¤ç”¨MinerUé€»è¾‘ï¼‰
    
    Args:
        doc_id: æ–‡æ¡£ID
        kb_id: çŸ¥è¯†åº“ID
        processor_result: DOTSç»Ÿä¸€å¤„ç†å™¨ç»“æœ
        update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
        embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        
    Returns:
        int: åˆ›å»ºçš„æ–‡æ¡£å—æ•°é‡
    """
    try:
        integration = RAGFlowIntegration(kb_id, doc_id, embedding_config)
        result = integration.process_and_store(processor_result, update_progress)
        
        if result['success']:
            is_parent_child = result.get('is_parent_child', False)
            chunk_info = f"{result['chunk_count']} ä¸ªå—"
            if is_parent_child:
                chunk_info += f" (çˆ¶å—:{result.get('total_parents', 0)}, å­å—:{result.get('total_children', 0)})"
            
            logger.info(f"DOTSç»Ÿä¸€RAGFlowèµ„æºåˆ›å»ºæˆåŠŸ: {chunk_info}")
            return result['chunk_count']
        else:
            logger.error(f"DOTSç»Ÿä¸€RAGFlowèµ„æºåˆ›å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
            return 0
            
    except Exception as e:
        logger.error(f"åˆ›å»ºDOTS RAGFlowèµ„æºå¼‚å¸¸: {e}")
        # ç¡®ä¿é”™è¯¯çŠ¶æ€è¢«è®°å½•
        try:
            update_document_progress(
                doc_id, 
                progress=1.0, 
                message=f"DOTSå¤„ç†å¼‚å¸¸: {str(e)}",
                status="0"
            )
        except:
            pass
        return 0