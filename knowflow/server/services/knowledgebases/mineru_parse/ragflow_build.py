from ragflow_sdk import RAGFlow
import os
import time
import shutil
import json
from dotenv import load_dotenv
from .minio_server import upload_directory_to_minio
from .mineru_test import update_markdown_image_urls
from .utils import split_markdown_to_chunks_configured, get_bbox_for_chunk, should_cleanup_temp_files
from ..utils import _get_kb_tenant_id, _get_tenant_api_key, _validate_base_url
from database import get_db_connection


# æ€§èƒ½ä¼˜åŒ–é…ç½®å‚æ•°
CHUNK_PROCESSING_CONFIG = {
    'enable_performance_stats': False,     # æ˜¯å¦å¯ç”¨æ€§èƒ½ç»Ÿè®¡
}

def _upload_images(kb_id, image_dir, update_progress):
    update_progress(0.7, "ä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    print(f"ç¬¬4æ­¥ï¼šä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    upload_directory_to_minio(kb_id, image_dir)

def get_ragflow_doc(doc_id, kb_id):
    """è·å–RAGFlowæ–‡æ¡£å¯¹è±¡å’Œdatasetå¯¹è±¡"""
    # é¦–å…ˆè·å–çŸ¥è¯†åº“çš„tenant_id
    tenant_id = _get_kb_tenant_id(kb_id)
    if not tenant_id:
        raise Exception(f"æ— æ³•è·å–çŸ¥è¯†åº“ {kb_id} çš„tenant_id")
    
    # æ ¹æ®tenant_idè·å–å¯¹åº”çš„API key
    api_key = _get_tenant_api_key(tenant_id)
    if not api_key:
        raise Exception(f"æ— æ³•è·å–tenant {tenant_id} çš„API key")
    
    base_url = _validate_base_url()
    rag_object = RAGFlow(api_key=api_key, base_url=base_url)
    datasets = rag_object.list_datasets(id=kb_id)
    if not datasets:
        raise Exception(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ {kb_id}")
    dataset = datasets[0]
    docs = dataset.list_documents(id=doc_id)
    if not docs:
        raise Exception(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_id}")
    return docs[0], dataset  # è¿”å›docå’Œdatasetå…ƒç»„

def _get_document_chunking_config(doc_id):
    """ä»æ•°æ®åº“è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT parser_config FROM document WHERE id = %s", (doc_id,))
        result = cursor.fetchone()
        
        if result and result[0]:
            parser_config = json.loads(result[0])
            chunking_config = parser_config.get('chunking_config')
            if chunking_config:
                return chunking_config
        
        return None
        
    except Exception as e:
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def add_chunks_with_enhanced_batch_api(doc, chunks, md_file_path, chunk_content_to_index, update_progress, parent_child_data=None, chunks_with_coordinates=None):
    """
    ä½¿ç”¨å¢å¼ºçš„batchæ¥å£å¤„ç†åˆ†å—ï¼ˆæ”¯æŒçˆ¶å­åˆ†å—å’Œåæ ‡ä¼ é€’ï¼‰
    
    Args:
        doc: RAGFlowæ–‡æ¡£å¯¹è±¡
        chunks: åˆ†å—å†…å®¹åˆ—è¡¨
        md_file_path: markdownæ–‡ä»¶è·¯å¾„
        chunk_content_to_index: åˆ†å—å†…å®¹åˆ°ç´¢å¼•çš„æ˜ å°„
        update_progress: è¿›åº¦æ›´æ–°å›è°ƒ
        parent_child_data: çˆ¶å­åˆ†å—æ•°æ®ï¼ˆå¯é€‰ï¼‰
        chunks_with_coordinates: åŒ…å«åæ ‡ä¿¡æ¯çš„åˆ†å—æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºDOTSç­‰æ²¡æœ‰mdæ–‡ä»¶çš„æƒ…å†µï¼‰
    
    Returns:
        int: æˆåŠŸæ·»åŠ çš„åˆ†å—æ•°é‡
    """
    
    if not chunks:
        update_progress(0.8, "æ²¡æœ‰chunkséœ€è¦æ·»åŠ ")
        return 0
    
    # åˆå§‹è¿›åº¦æ›´æ–°
    update_progress(0.8, "å¼€å§‹æ‰¹é‡æ·»åŠ chunksåˆ°æ–‡æ¡£ï¼ˆä½¿ç”¨å¢å¼ºbatchæ¥å£ï¼‰...")
    
    try:
        # å‡†å¤‡æ‰¹é‡æ•°æ®ï¼ŒåŒ…å«ä½ç½®ä¿¡æ¯
        batch_chunks = []
        for i, chunk in enumerate(chunks):
            if chunk and chunk.strip():
                chunk_data = {
                    "content": chunk.strip(),
                    "important_keywords": [],  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…³é”®è¯æå–
                    "questions": []  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é—®é¢˜ç”Ÿæˆ
                }
                
                # è·å–chunkçš„åŸå§‹ç´¢å¼•ï¼ˆç¡®ä¿æ’åºæ­£ç¡®æ€§ï¼‰
                original_index = chunk_content_to_index.get(chunk.strip(), i)
                
                # ç»Ÿä¸€æ’åºæœºåˆ¶ï¼šå›ºå®špage_num_int=1ï¼Œtop_int=åŸå§‹ç´¢å¼•
                chunk_data["page_num_int"] = [1]  # å›ºå®šä¸º1ï¼Œä¿è¯æ‰€æœ‰chunkséƒ½åœ¨åŒä¸€"é¡µ"
                chunk_data["top_int"] = original_index  # ä½¿ç”¨åŸå§‹ç´¢å¼•ä¿è¯é¡ºåº
                
                # å°è¯•è·å–ç²¾ç¡®ä½ç½®ä¿¡æ¯ï¼ˆä½œä¸ºé¢å¤–çš„ä½ç½®æ•°æ®ï¼Œä¸å½±å“æ’åºï¼‰
                position_found = False
                
                # ä¼˜å…ˆä»chunks_with_coordinatesè·å–åæ ‡ï¼ˆDOTSç­‰æƒ…å†µï¼‰
                if chunks_with_coordinates and i < len(chunks_with_coordinates):
                    chunk_with_coord = chunks_with_coordinates[i]
                    if chunk_with_coord and chunk_with_coord.get('positions'):
                        chunk_data["positions"] = chunk_with_coord['positions']
                        print(f"ğŸ“ chunk {original_index}: DOTSåæ ‡ ({len(chunk_with_coord['positions'])} ä¸ªä½ç½®) + ç´¢å¼•æ’åº (page=1, top={original_index})")
                        position_found = True
                
                # å¦‚æœæ²¡æœ‰ç›´æ¥åæ ‡ï¼Œå°è¯•ä»mdæ–‡ä»¶è·å–ï¼ˆMinerUæƒ…å†µï¼‰
                if not position_found and md_file_path is not None:
                    try:
                        position_int_temp = get_bbox_for_chunk(md_file_path, chunk.strip())
                        if position_int_temp is not None:
                            # æœ‰å®Œæ•´ä½ç½®ä¿¡æ¯æ—¶ï¼Œä»…æ·»åŠ positionsï¼Œä¸è¦†ç›–æ’åºå­—æ®µ
                            chunk_data["positions"] = position_int_temp
                            print(f"ğŸ“ chunk {original_index}: æ‰¾åˆ°ç²¾ç¡®åæ ‡ ({len(position_int_temp)} ä¸ªä½ç½®) + ç´¢å¼•æ’åº (page=1, top={original_index})")
                            position_found = True
                        else:
                            print(f"ğŸ“ chunk {original_index}: ä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={original_index})")
                    except Exception as pos_e:
                        print(f"ğŸ“ chunk {original_index}: åæ ‡è·å–å¼‚å¸¸ï¼Œä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={original_index})")
                
                # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°åæ ‡
                if not position_found:
                    if md_file_path is None and chunks_with_coordinates is None:
                        print(f"ğŸ“ chunk {original_index}: æ— MDæ–‡ä»¶å’Œåæ ‡æ•°æ®ï¼Œä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={original_index})")
                    elif chunks_with_coordinates is None:
                        print(f"ğŸ“ chunk {original_index}: æ— åæ ‡æ•°æ®ï¼Œä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={original_index})")
                    else:
                        print(f"ğŸ“ chunk {original_index}: åæ ‡æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨ç´¢å¼•æ’åº (page=1, top={original_index})")
                
                batch_chunks.append(chunk_data)
        
        if not batch_chunks:
            update_progress(0.95, "æ²¡æœ‰æœ‰æ•ˆçš„chunks")
            return 0
        
        print(f"ğŸ“¦ å‡†å¤‡è°ƒç”¨å¢å¼ºçš„batchæ¥å£å¤„ç† {len(batch_chunks)} ä¸ªæœ‰æ•ˆchunks")
        
        # è°ƒç”¨å¢å¼ºçš„batchæ¥å£
        import requests
        import json
        
        # è·å–APIåŸºæœ¬ä¿¡æ¯
        base_url = doc.rag.api_url
        headers = doc.rag.authorization_header
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "chunks": batch_chunks,
            "batch_size": 20
        }
        
        # å¦‚æœæœ‰çˆ¶å­åˆ†å—æ•°æ®ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if parent_child_data:
            request_data["parent_child_data"] = parent_child_data
            print(f"ğŸ”— [INFO] æ·»åŠ çˆ¶å­åˆ†å—æ•°æ®åˆ°batchè¯·æ±‚: {len(parent_child_data.get('parent_chunks', []))} çˆ¶åˆ†å—, {len(parent_child_data.get('relationships', []))} æ˜ å°„å…³ç³»")
        
        # è°ƒç”¨å¢å¼ºçš„batchæ¥å£
        api_url = f"{base_url}/datasets/{doc.dataset_id}/documents/{doc.id}/chunks/batch"
        print(f"ğŸ”— å‘é€å¢å¼ºbatchè¯·æ±‚åˆ°: {api_url}")
        
        response = requests.post(api_url, json=request_data, headers=headers)
        
        print(f"ğŸ“¥ å¢å¼ºbatchæ¥å£å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            try:
                result = response.json()
                if result.get("code") == 0:
                    # æ‰¹é‡æ·»åŠ æˆåŠŸ
                    data = result.get("data", {})
                    added = data.get("total_added", 0)
                    failed = data.get("total_failed", 0)
                    
                    print(f"âœ… å¢å¼ºbatchæ¥å£å¤„ç†å®Œæˆ: æˆåŠŸ {added} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª")
                    
                    if parent_child_data:
                        print(f"ğŸ”— çˆ¶å­åˆ†å—å¤„ç†ä¹Ÿå·²å®Œæˆ")
                    
                    update_progress(0.95, f"batchå¤„ç†å®Œæˆ: æˆåŠŸ {added}/{len(batch_chunks)} chunks")
                    return added
                else:
                    # æ‰¹é‡æ·»åŠ å¤±è´¥
                    error_msg = result.get("message", "Unknown error")
                    print(f"âŒ å¢å¼ºbatchæ¥å£å¤±è´¥: {error_msg}")
                    update_progress(0.95, f"batchå¤„ç†å¤±è´¥: {error_msg}")
                    return 0
            except json.JSONDecodeError:
                print(f"âŒ å¢å¼ºbatchæ¥å£å“åº”è§£æå¤±è´¥")
                update_progress(0.95, "å“åº”è§£æå¤±è´¥")
                return 0
        else:
            print(f"âŒ å¢å¼ºbatchæ¥å£HTTPé”™è¯¯: {response.status_code}")
            update_progress(0.95, f"HTTPé”™è¯¯: {response.status_code}")
            return 0
        
    except Exception as e:
        update_progress(0.95, f"å¢å¼ºbatchå¤„ç†å¼‚å¸¸: {str(e)}")
        print(f"âŒ å¢å¼ºbatchå¤„ç†å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return 0



def _cleanup_temp_files(md_file_path):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if not should_cleanup_temp_files():
        return
    
    try:
        temp_dir = os.path.dirname(os.path.abspath(md_file_path))
        shutil.rmtree(temp_dir)
    except Exception as e:
        pass

def create_ragflow_resources(doc_id, kb_id, md_file_path, image_dir, update_progress):
    """
    ä½¿ç”¨å¢å¼ºæ–‡æœ¬åˆ›å»ºRAGFlowçŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹
    """
    try:
        doc, dataset = get_ragflow_doc(doc_id, kb_id)

        _upload_images(kb_id, image_dir, update_progress)

        # è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®
        chunking_config = _get_document_chunking_config(doc_id)
        
        enhanced_text = update_markdown_image_urls(md_file_path, kb_id)

        # ä¿å­˜åŸå§‹markdownåˆ°æœ¬åœ°ç”¨äºè°ƒè¯•
        try:
            debug_md_path = f"/tmp/debug_markdown_{doc_id}_{kb_id}.md"
            with open(debug_md_path, 'w', encoding='utf-8') as f:
                f.write(enhanced_text)
            print(f"ğŸ” [DEBUG] åŸå§‹markdownå·²ä¿å­˜åˆ°: {debug_md_path}")
        except Exception as e:
            pass
        
        # ä¼ é€’åˆ†å—é…ç½®ç»™åˆ†å—å‡½æ•°
        if chunking_config:
            chunks = split_markdown_to_chunks_configured(
                enhanced_text, 
                chunk_token_num=chunking_config.get('chunk_token_num', 256),
                min_chunk_tokens=chunking_config.get('min_chunk_tokens', 10),
                chunking_config=chunking_config,
                doc_id=doc_id,
                kb_id=kb_id
            )
        else:
            chunks = split_markdown_to_chunks_configured(enhanced_text, chunk_token_num=256)
        
        # å‡†å¤‡çˆ¶å­åˆ†å—æ•°æ®ï¼ˆå¦‚æœä½¿ç”¨äº†çˆ¶å­åˆ†å—ç­–ç•¥ï¼‰
        parent_child_data = None
        is_parent_child = (chunking_config and 
                          chunking_config.get('strategy') == 'parent_child')
        
        if is_parent_child:
            # è·å–çˆ¶å­åˆ†å—çš„è¯¦ç»†ç»“æœ
            from .utils import get_last_parent_child_result
            parent_child_result = get_last_parent_child_result()
            
            if parent_child_result:
                print(f"ğŸ¯ [INFO] æ£€æµ‹åˆ°çˆ¶å­åˆ†å—ç­–ç•¥ï¼Œå°†ä½¿ç”¨å¢å¼ºçš„batchæ¥å£å¤„ç†")
                print(f"  ğŸ‘¨ çˆ¶åˆ†å—æ•°: {parent_child_result.get('total_parents', 0)}")
                print(f"  ğŸ‘¶ å­åˆ†å—æ•°: {parent_child_result.get('total_children', 0)}")
                
                # å‡†å¤‡çˆ¶å­åˆ†å—æ•°æ®
                parent_child_data = {
                    'doc_id': doc_id,
                    'kb_id': kb_id,
                    'parent_chunks': parent_child_result.get('parent_chunks', []),
                    'child_chunks': parent_child_result.get('child_chunks', []),
                    'relationships': parent_child_result.get('relationships', [])
                }
                
                # å¯¹äºçˆ¶å­åˆ†å—ï¼Œä½¿ç”¨å­åˆ†å—å†…å®¹
                chunks = [chunk['content'] for chunk in parent_child_data['child_chunks']]
        
        # ç»Ÿä¸€åˆ†å—å¤„ç† - ä¼˜åŒ–åç»Ÿä¸€ä½¿ç”¨å¢å¼ºçš„batchæ¥å£ï¼ˆæ”¯æŒçˆ¶å­åˆ†å—å’Œæ ‡å‡†åˆ†å—ï¼‰
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks)}
        chunk_count = add_chunks_with_enhanced_batch_api(doc, chunks, md_file_path, chunk_content_to_index, update_progress, parent_child_data=parent_child_data)
        # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        _cleanup_temp_files(md_file_path)

        # ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°100%
        update_progress(1.0, f"å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {chunk_count} ä¸ªchunks")
        return chunk_count

    except Exception as e:
        import traceback
        traceback.print_exc()

        try:
            update_progress(1.0, f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        except Exception as progress_e:
            pass
        
        raise
