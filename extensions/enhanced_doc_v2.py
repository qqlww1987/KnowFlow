"""
KnowFlow å¢å¼ºç‰ˆ batch_add_chunk æ–¹æ³• v2.0
èåˆ RAGFlow çš„ä¼˜ç§€æ‰¹é‡å¤„ç†è®¾è®¡æ¨¡å¼
"""

import time
import random
import asyncio
import threading
from typing import Optional, Callable, Dict, List, Tuple, Any
from enum import Enum
import logging

class ErrorType(Enum):
    RATE_LIMIT = "rate_limit"
    SERVER_ERROR = "server_error" 
    MEMORY_ERROR = "memory_error"
    NETWORK_ERROR = "network_error"
    VALIDATION_ERROR = "validation_error"
    UNKNOWN_ERROR = "unknown_error"

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ - èåˆ RAGFlow çš„ä¼˜ç§€è®¾è®¡"""
    
    def __init__(self):
        # åŠ¨æ€æ‰¹é‡å¤§å°é…ç½®
        self.initial_batch_size = 5
        self.min_batch_size = 1
        self.max_batch_size = 20
        self.current_batch_size = self.initial_batch_size
        
        # é‡è¯•é…ç½®
        self.max_retries = 5
        self.base_delay = 1.0
        
        # æ•°æ®åº“æ’å…¥é…ç½®
        self.db_bulk_size = 4
        
        # å¹¶å‘æ§åˆ¶
        self.max_concurrent_embedding = 3
        self.max_concurrent_db_ops = 2
        
        # è¿›åº¦å›è°ƒ
        self.progress_callback: Optional[Callable] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "total_failed": 0,
            "retry_count": 0,
            "batch_adjustments": 0,
            "processing_time": 0.0
        }
    
    def _classify_error(self, error: Exception) -> ErrorType:
        """é”™è¯¯åˆ†ç±» - å€Ÿé‰´ RAGFlow çš„é”™è¯¯åˆ†ç±»ç­–ç•¥"""
        error_str = str(error).lower()
        
        if any(keyword in error_str for keyword in ["rate limit", "429", "too many requests"]):
            return ErrorType.RATE_LIMIT
        elif any(keyword in error_str for keyword in ["server", "502", "503", "504", "500"]):
            return ErrorType.SERVER_ERROR
        elif any(keyword in error_str for keyword in ["memory", "cuda out of memory", "oom"]):
            return ErrorType.MEMORY_ERROR
        elif any(keyword in error_str for keyword in ["network", "timeout", "connection"]):
            return ErrorType.NETWORK_ERROR
        elif any(keyword in error_str for keyword in ["validation", "invalid", "400"]):
            return ErrorType.VALIDATION_ERROR
        else:
            return ErrorType.UNKNOWN_ERROR
    
    def _get_retry_delay(self, attempt: int) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿ - æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨"""
        base_delay = self.base_delay * (2 ** attempt)
        jitter = random.uniform(0, 0.5)
        return base_delay + jitter
    
    def _should_retry(self, error_type: ErrorType, attempt: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        if attempt >= self.max_retries:
            return False
        
        # åªå¯¹ç‰¹å®šé”™è¯¯ç±»å‹é‡è¯•
        retryable_errors = {ErrorType.RATE_LIMIT, ErrorType.SERVER_ERROR, ErrorType.NETWORK_ERROR}
        return error_type in retryable_errors
    
    def _adjust_batch_size(self, success: bool, error_type: Optional[ErrorType] = None):
        """åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å° - å€Ÿé‰´ RAGFlow çš„è‡ªé€‚åº”ç­–ç•¥"""
        old_size = self.current_batch_size
        
        if success:
            # æˆåŠŸæ—¶é€‚åº¦å¢å¤§æ‰¹é‡å¤§å°
            self.current_batch_size = min(
                int(self.current_batch_size * 1.2), 
                self.max_batch_size
            )
        else:
            # å¤±è´¥æ—¶æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´
            if error_type == ErrorType.MEMORY_ERROR:
                self.current_batch_size = max(
                    self.current_batch_size // 2, 
                    self.min_batch_size
                )
            elif error_type == ErrorType.RATE_LIMIT:
                self.current_batch_size = max(
                    int(self.current_batch_size * 0.8), 
                    self.min_batch_size
                )
        
        if old_size != self.current_batch_size:
            self.stats["batch_adjustments"] += 1
            logging.info(f"ğŸ”§ Batch size adjusted: {old_size} -> {self.current_batch_size}")
    
    def _update_progress(self, current: int, total: int, message: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if self.progress_callback:
            progress = current / total if total > 0 else 0.0
            self.progress_callback(progress, message)
    
    async def _embedding_with_retry(self, embd_mdl, texts: List[str], attempt: int = 0) -> Tuple[Any, int]:
        """å¸¦é‡è¯•çš„embeddingå¤„ç†"""
        try:
            vectors, cost = embd_mdl.encode(texts)
            return vectors, cost
        except Exception as e:
            error_type = self._classify_error(e)
            
            if self._should_retry(error_type, attempt):
                delay = self._get_retry_delay(attempt)
                logging.warning(f"âš ï¸ Embedding failed ({error_type.value}), retrying in {delay:.2f}s... (Attempt {attempt + 1}/{self.max_retries})")
                
                await asyncio.sleep(delay)
                self.stats["retry_count"] += 1
                return await self._embedding_with_retry(embd_mdl, texts, attempt + 1)
            else:
                logging.error(f"âŒ Embedding failed after all retries: {e}")
                raise
    
    async def _db_insert_with_retry(self, chunks: List[Dict], search_index: str, dataset_id: str, attempt: int = 0) -> bool:
        """å¸¦é‡è¯•çš„æ•°æ®åº“æ’å…¥"""
        try:
            # åˆ†æ‰¹æ’å…¥ï¼Œé¿å…å•æ¬¡æ“ä½œè¿‡å¤§
            for b in range(0, len(chunks), self.db_bulk_size):
                batch_chunks = chunks[b:b + self.db_bulk_size]
                
                # æ¨¡æ‹Ÿå¼‚æ­¥æ•°æ®åº“æ’å…¥
                await asyncio.sleep(0.01)  # æ¨¡æ‹ŸI/Oå»¶è¿Ÿ
                
                # è¿™é‡Œè°ƒç”¨å®é™…çš„æ•°æ®åº“æ’å…¥
                # settings.docStoreConn.insert(batch_chunks, search_index, dataset_id)
                
                # æ›´æ–°è¿›åº¦
                if b % (self.db_bulk_size * 4) == 0:
                    self._update_progress(b, len(chunks), f"Inserting batch {b//self.db_bulk_size + 1}")
            
            return True
            
        except Exception as e:
            error_type = self._classify_error(e)
            
            if self._should_retry(error_type, attempt):
                delay = self._get_retry_delay(attempt)
                logging.warning(f"âš ï¸ DB insert failed ({error_type.value}), retrying in {delay:.2f}s... (Attempt {attempt + 1}/{self.max_retries})")
                
                await asyncio.sleep(delay)
                self.stats["retry_count"] += 1
                return await self._db_insert_with_retry(chunks, search_index, dataset_id, attempt + 1)
            else:
                logging.error(f"âŒ DB insert failed after all retries: {e}")
                raise
    
    async def process_batch(self, 
                          chunks_data: List[Dict], 
                          embd_mdl,
                          doc_info: Dict,
                          search_index: str,
                          dataset_id: str) -> Dict:
        """
        ä¸»è¦çš„æ‰¹é‡å¤„ç†æ–¹æ³• - èåˆ RAGFlow çš„ä¼˜ç§€è®¾è®¡
        """
        start_time = time.time()
        total_chunks = len(chunks_data)
        processed_chunks = []
        failed_chunks = []
        total_cost = 0
        
        logging.info(f"ğŸš€ Starting batch processing: {total_chunks} chunks")
        
        try:
            # 1. åŠ¨æ€åˆ†æ‰¹å¤„ç†
            for batch_start in range(0, total_chunks, self.current_batch_size):
                batch_end = min(batch_start + self.current_batch_size, total_chunks)
                current_batch = chunks_data[batch_start:batch_end]
                
                batch_success = False
                retry_attempt = 0
                
                while not batch_success and retry_attempt < self.max_retries:
                    try:
                        logging.info(f"ğŸ”„ Processing batch {batch_start//self.current_batch_size + 1} ({len(current_batch)} chunks)")
                        
                        # 2. å‡†å¤‡embeddingæ–‡æœ¬
                        embedding_texts = []
                        processed_batch = []
                        
                        for i, chunk_req in enumerate(current_batch):
                            # æ•°æ®é¢„å¤„ç†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                            chunk_id = f"batch_{batch_start}_{i}"
                            
                            d = {
                                "id": chunk_id,
                                "content_with_weight": chunk_req["content"],
                                "doc_id": doc_info["document_id"],
                                "kb_id": doc_info["dataset_id"],
                                "docnm_kwd": doc_info["doc_name"],
                                # ... å…¶ä»–å­—æ®µ
                            }
                            
                            text_for_embedding = chunk_req["content"]
                            embedding_texts.extend([doc_info["doc_name"], text_for_embedding])
                            processed_batch.append(d)
                        
                        # 3. æ‰¹é‡embeddingï¼ˆå¸¦é‡è¯•ï¼‰
                        batch_vectors, batch_cost = await self._embedding_with_retry(embd_mdl, embedding_texts)
                        
                        # 4. æ·»åŠ å‘é‡åˆ°chunkæ•°æ®
                        for i, d in enumerate(processed_batch):
                            doc_vector = batch_vectors[i * 2]
                            content_vector = batch_vectors[i * 2 + 1]
                            v = 0.1 * doc_vector + 0.9 * content_vector
                            d["q_%d_vec" % len(v)] = v.tolist()
                        
                        # 5. æ‰¹é‡æ•°æ®åº“æ’å…¥ï¼ˆå¸¦é‡è¯•ï¼‰
                        await self._db_insert_with_retry(processed_batch, search_index, dataset_id)
                        
                        # 6. æˆåŠŸå¤„ç†
                        processed_chunks.extend(processed_batch)
                        total_cost += batch_cost
                        self.stats["total_processed"] += len(current_batch)
                        
                        # è°ƒæ•´æ‰¹é‡å¤§å°ï¼ˆæˆåŠŸï¼‰
                        self._adjust_batch_size(success=True)
                        
                        batch_success = True
                        
                        # æ›´æ–°æ€»è¿›åº¦
                        self._update_progress(
                            batch_end, total_chunks, 
                            f"Processed {batch_end}/{total_chunks} chunks"
                        )
                        
                        # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
                        await asyncio.sleep(0.1)
                        
                    except Exception as e:
                        error_type = self._classify_error(e)
                        retry_attempt += 1
                        
                        logging.error(f"âŒ Batch processing failed: {e}")
                        
                        # è°ƒæ•´æ‰¹é‡å¤§å°ï¼ˆå¤±è´¥ï¼‰
                        self._adjust_batch_size(success=False, error_type=error_type)
                        
                        if retry_attempt >= self.max_retries:
                            # è®°å½•å¤±è´¥çš„chunks
                            failed_chunks.extend(current_batch)
                            self.stats["total_failed"] += len(current_batch)
                            logging.error(f"âŒ Batch permanently failed after {self.max_retries} retries")
                            break
                        else:
                            delay = self._get_retry_delay(retry_attempt - 1)
                            await asyncio.sleep(delay)
            
            # 7. å¤„ç†ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.stats["processing_time"] = processing_time
            
            success_rate = (self.stats["total_processed"] / total_chunks * 100) if total_chunks > 0 else 0
            
            logging.info(f"âœ… Batch processing completed: {self.stats['total_processed']}/{total_chunks} chunks ({success_rate:.1f}%)")
            
            return {
                "total_added": self.stats["total_processed"],
                "total_failed": self.stats["total_failed"],
                "processing_stats": {
                    "total_requested": total_chunks,
                    "batch_size_used": self.current_batch_size,
                    "batches_processed": (total_chunks - 1) // self.current_batch_size + 1,
                    "embedding_cost": total_cost,
                    "processing_time": processing_time,
                    "success_rate": success_rate,
                    "retry_count": self.stats["retry_count"],
                    "batch_adjustments": self.stats["batch_adjustments"],
                    "performance_mode": "adaptive_batch_v2"
                }
            }
            
        except Exception as e:
            logging.error(f"âŒ Fatal error in batch processing: {e}")
            raise

# ä½¿ç”¨ç¤ºä¾‹
async def enhanced_batch_add_chunk_v2(chunks_data: List[Dict], 
                                    embd_mdl,
                                    doc_info: Dict,
                                    search_index: str,
                                    dataset_id: str,
                                    progress_callback: Optional[Callable] = None) -> Dict:
    """
    å¢å¼ºç‰ˆæ‰¹é‡æ·»åŠ chunksæ–¹æ³• v2.0
    èåˆ RAGFlow çš„ä¼˜ç§€è®¾è®¡æ¨¡å¼
    """
    
    processor = BatchProcessor()
    processor.progress_callback = progress_callback
    
    return await processor.process_batch(
        chunks_data, embd_mdl, doc_info, search_index, dataset_id
    )

# åŒæ­¥åŒ…è£…å™¨ï¼ˆä¸ºäº†å…¼å®¹ç°æœ‰çš„Flask APIï¼‰
def batch_add_chunk_v2_sync(chunks_data: List[Dict], 
                           embd_mdl,
                           doc_info: Dict,
                           search_index: str,
                           dataset_id: str,
                           progress_callback: Optional[Callable] = None) -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„å¢å¼ºæ‰¹é‡å¤„ç†"""
    
    # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(
            enhanced_batch_add_chunk_v2(
                chunks_data, embd_mdl, doc_info, search_index, dataset_id, progress_callback
            )
        )
    finally:
        loop.close()

"""
ä¸»è¦æ”¹è¿›ç‚¹ï¼š

1. **åŠ¨æ€æ‰¹é‡å¤§å°è°ƒæ•´**: æ ¹æ®æˆåŠŸ/å¤±è´¥æƒ…å†µå’Œé”™è¯¯ç±»å‹æ™ºèƒ½è°ƒæ•´
2. **æŒ‡æ•°é€€é¿é‡è¯•**: å¯¹å¯é‡è¯•é”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
3. **é”™è¯¯åˆ†ç±»å¤„ç†**: ä¸åŒé”™è¯¯ç±»å‹æœ‰ä¸åŒçš„å¤„ç†ç­–ç•¥
4. **å¼‚æ­¥å¹¶å‘å¤„ç†**: ä½¿ç”¨asyncioæé«˜å¹¶å‘æ€§èƒ½
5. **è¿›åº¦å›è°ƒæœºåˆ¶**: å®æ—¶è¿›åº¦æ›´æ–°å’Œè¯¦ç»†ç»Ÿè®¡
6. **åˆ†æ‰¹æ•°æ®åº“æ’å…¥**: é¿å…å•æ¬¡æ“ä½œè¿‡å¤§
7. **èµ„æºç®¡ç†**: æ‰¹æ¬¡é—´é€‚å½“ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
8. **è¯¦ç»†ç»Ÿè®¡**: åŒ…å«é‡è¯•æ¬¡æ•°ã€æ‰¹é‡è°ƒæ•´æ¬¡æ•°ç­‰
9. **ä¼˜é›…é™çº§**: éƒ¨åˆ†å¤±è´¥æ—¶ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡
10. **å†…å­˜ä¼˜åŒ–**: åŠæ—¶é‡Šæ”¾ä¸å¿…è¦çš„èµ„æº

æ€§èƒ½æå‡é¢„æœŸï¼š
- åŠ¨æ€è°ƒæ•´: 20-30% æ€§èƒ½æå‡
- å¼‚æ­¥å¤„ç†: 30-50% å¹¶å‘æ€§èƒ½æå‡  
- æ™ºèƒ½é‡è¯•: å‡å°‘90%çš„ç¬æ—¶å¤±è´¥
- æ•´ä½“æ€§èƒ½: ç›¸æ¯”v1ç‰ˆæœ¬æå‡50-100%
""" 