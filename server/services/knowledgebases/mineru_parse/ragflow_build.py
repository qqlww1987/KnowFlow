from ragflow_sdk import RAGFlow
import os
import time
import shutil
import json
from dotenv import load_dotenv
from .minio_server import upload_directory_to_minio
from .mineru_test import update_markdown_image_urls
from .utils import split_markdown_to_chunks_configured, get_bbox_for_chunk, update_document_progress, should_cleanup_temp_files
from database import get_es_client, get_db_connection

def _validate_environment():
    """éªŒè¯ç¯å¢ƒå˜é‡é…ç½®"""
    load_dotenv()
    api_key = os.getenv('RAGFLOW_API_KEY')
    base_url = os.getenv('RAGFLOW_BASE_URL')
    if not api_key:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_API_KEYæˆ–ä½¿ç”¨--api_keyå‚æ•°æŒ‡å®šã€‚")
    if not base_url:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_BASE_URLæˆ–ä½¿ç”¨--server_ipå‚æ•°æŒ‡å®šã€‚")
    return api_key, base_url

def _upload_images(kb_id, image_dir, update_progress):
    update_progress(0.7, "ä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    print(f"ç¬¬4æ­¥ï¼šä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    upload_directory_to_minio(kb_id, image_dir)

def get_ragflow_doc(doc_id, kb_id):
    """è·å–RAGFlowæ–‡æ¡£å¯¹è±¡"""
    api_key, base_url = _validate_environment()
    rag_object = RAGFlow(api_key=api_key, base_url=base_url)
    datasets = rag_object.list_datasets(id=kb_id)
    if not datasets:
        raise Exception(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ {kb_id}")
    dataset = datasets[0]
    docs = dataset.list_documents(id=doc_id)
    if not docs:
        raise Exception(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_id}")
    return docs[0]

def _get_document_chunking_config(doc_id):
    """ä»æ•°æ®åº“è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT parser_config FROM document WHERE id = %s", (doc_id,))
        result = cursor.fetchone()
        
        if result and result[0]:
            parser_config = json.loads(result[0])
            chunking_config = parser_config.get('chunking_config')
            if chunking_config:
                print(f"ğŸ”§ [DEBUG] ä»æ•°æ®åº“è·å–åˆ°åˆ†å—é…ç½®: {chunking_config}")
                return chunking_config
        
        print(f"ğŸ“„ [DEBUG] æ–‡æ¡£ {doc_id} æ²¡æœ‰è‡ªå®šä¹‰åˆ†å—é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
        
    except Exception as e:
        print(f"âš ï¸ [WARNING] è·å–æ–‡æ¡£åˆ†å—é…ç½®å¤±è´¥: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def add_chunks_to_doc(doc, chunks, update_progress):
    update_progress(0.8, "æ·»åŠ  chunk åˆ°æ–‡æ¡£...")
    print(f"æ€»å…±æ¥æ”¶åˆ° {len(chunks)} ä¸ª chunks å‡†å¤‡æ·»åŠ ã€‚")
    for i, chunk in enumerate(chunks):
        chunk_preview = chunk.strip()[:50].replace('\n', ' ')
        print(f"å‡†å¤‡æ·»åŠ  Chunk {i}: \"{chunk_preview}...\"")
        if chunk and chunk.strip():
            try:
                doc.add_chunk(content=chunk)
            except Exception as e:
                print(f"æ·»åŠ  chunk å¤±è´¥: {e}")

def _update_chunks_position(doc, md_file_path, chunk_content_to_index):
    es_client = get_es_client()
    print(f"æ–‡æ¡£: id: {doc.id})")
    chunk_count = 0
    tenant_id = doc.created_by
    index_name = f"ragflow_{tenant_id}"
    for chunk in doc.list_chunks(keywords=None, page=1, page_size=10000):
        original_index = chunk_content_to_index.get(chunk.content)
        if original_index is None:
            print(f"è­¦å‘Š: æ— æ³•ä¸ºå— id={chunk.id} çš„å†…å®¹æ‰¾åˆ°åŸå§‹ç´¢å¼•ï¼Œå°†è·³è¿‡æ­¤å—ã€‚")
            continue
        
        direct_update = {
            "doc": {
                "top_int": original_index
            }
        }
        
        # å°è¯•è·å–ä½ç½®ä¿¡æ¯ï¼Œå¦‚æœæˆåŠŸåˆ™æ·»åŠ åˆ°æ›´æ–°ä¸­
        try:
            position_int_temp = get_bbox_for_chunk(md_file_path, chunk.content)
            if position_int_temp is not None:
                doc_fields = {}
                _add_positions(doc_fields, position_int_temp)
                direct_update["doc"]["position_int"] = doc_fields.get("position_int")
        except Exception as e:
            print(f"è·å–chunkä½ç½®å¼‚å¸¸: {e}")
        
        # æ‰§è¡ŒESæ›´æ–°
        try:
            es_client.update(index=index_name, id=chunk.id, body=direct_update, refresh=True)
            chunk_count += 1
        except Exception as es_e:
            print(f"ESæ›´æ–°å¼‚å¸¸: {es_e}")
        
    return chunk_count

def _cleanup_temp_files(md_file_path):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if not should_cleanup_temp_files():
        print(f"[INFO] é…ç½®ä¸ºä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œè·¯å¾„: {os.path.dirname(os.path.abspath(md_file_path))}")
        return
    
    try:
        temp_dir = os.path.dirname(os.path.abspath(md_file_path))
        shutil.rmtree(temp_dir)
        print(f"[INFO] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç›®å½•: {temp_dir}")
    except Exception as e:
        print(f"[WARNING] æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¼‚å¸¸: {e}")

def create_ragflow_resources(doc_id, kb_id, md_file_path, image_dir, update_progress):
    """
    ä½¿ç”¨å¢å¼ºæ–‡æœ¬åˆ›å»ºRAGFlowçŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹
    """
    try:
        doc = get_ragflow_doc(doc_id, kb_id)

        _upload_images(kb_id, image_dir, update_progress)

        # è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®
        chunking_config = _get_document_chunking_config(doc_id)
        
        enhanced_text = update_markdown_image_urls(md_file_path, kb_id)
        
        # ä¼ é€’åˆ†å—é…ç½®ç»™åˆ†å—å‡½æ•°
        if chunking_config:
            chunks = split_markdown_to_chunks_configured(
                enhanced_text, 
                chunk_token_num=chunking_config.get('chunk_token_num', 256),
                min_chunk_tokens=chunking_config.get('min_chunk_tokens', 10),
                chunking_config=chunking_config
            )
        else:
            chunks = split_markdown_to_chunks_configured(enhanced_text, chunk_token_num=256)
        
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks)}

        add_chunks_to_doc(doc, chunks, update_progress)
        chunk_count = _update_chunks_position(doc, md_file_path, chunk_content_to_index)
        # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        _cleanup_temp_files(md_file_path)

        return chunk_count

    except Exception as e:
        print(f"create_ragflow_resources å¤„ç†å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def _add_positions(d, poss):
    try:
        if not poss:
            return
        page_num_int = []
        position_int = []
        top_int = []
        for pn, left, right, top, bottom in poss:
            page_num_int.append(int(pn + 1))
            top_int.append(int(top))
            position_int.append((int(pn + 1), int(left), int(right), int(top), int(bottom)))
        d["page_num_int"] = page_num_int
        d["position_int"] = position_int
        d["top_int"] = top_int
    except Exception as e:
        print(f"add_positionså¼‚å¸¸: {e}")