from ragflow_sdk import RAGFlow
import os
import time
import shutil
import json
from dotenv import load_dotenv
from .minio_server import upload_directory_to_minio
from .mineru_test import update_markdown_image_urls
from .utils import split_markdown_to_chunks_configured, get_bbox_for_chunk, update_document_progress, should_cleanup_temp_files
from database import get_es_client, get_db_connection
import concurrent.futures
import threading
from datetime import datetime

# æ€§èƒ½ä¼˜åŒ–é…ç½®å‚æ•°
CHUNK_PROCESSING_CONFIG = {
    'max_concurrent_workers': 8,           # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
    'es_bulk_batch_size': 100,           # ESæ‰¹é‡æ“ä½œæ‰¹æ¬¡å¤§å°
    'enable_concurrent_chunk_add': True,   # æ˜¯å¦å¯ç”¨å¹¶å‘æ·»åŠ chunks
    'chunk_add_timeout': 30,              # å•ä¸ªchunkæ·»åŠ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'es_bulk_timeout': 60,                # ESæ‰¹é‡æ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'enable_performance_stats': True,     # æ˜¯å¦å¯ç”¨æ€§èƒ½ç»Ÿè®¡
}

def _validate_environment():
    """éªŒè¯ç¯å¢ƒå˜é‡é…ç½®"""
    load_dotenv()
    api_key = os.getenv('RAGFLOW_API_KEY')
    base_url = os.getenv('RAGFLOW_BASE_URL')
    if not api_key:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_API_KEYæˆ–ä½¿ç”¨--api_keyå‚æ•°æŒ‡å®šã€‚")
    if not base_url:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_BASE_URLæˆ–ä½¿ç”¨--server_ipå‚æ•°æŒ‡å®šã€‚")
    return api_key, base_url

def _upload_images(kb_id, image_dir, update_progress):
    update_progress(0.7, "ä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    print(f"ç¬¬4æ­¥ï¼šä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    upload_directory_to_minio(kb_id, image_dir)

def get_ragflow_doc(doc_id, kb_id):
    """è·å–RAGFlowæ–‡æ¡£å¯¹è±¡"""
    api_key, base_url = _validate_environment()
    rag_object = RAGFlow(api_key=api_key, base_url=base_url)
    datasets = rag_object.list_datasets(id=kb_id)
    if not datasets:
        raise Exception(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ {kb_id}")
    dataset = datasets[0]
    docs = dataset.list_documents(id=doc_id)
    if not docs:
        raise Exception(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_id}")
    return docs[0]

def _get_document_chunking_config(doc_id):
    """ä»æ•°æ®åº“è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT parser_config FROM document WHERE id = %s", (doc_id,))
        result = cursor.fetchone()
        
        if result and result[0]:
            parser_config = json.loads(result[0])
            chunking_config = parser_config.get('chunking_config')
            if chunking_config:
                print(f"ğŸ”§ [DEBUG] ä»æ•°æ®åº“è·å–åˆ°åˆ†å—é…ç½®: {chunking_config}")
                return chunking_config
        
        print(f"ğŸ“„ [DEBUG] æ–‡æ¡£ {doc_id} æ²¡æœ‰è‡ªå®šä¹‰åˆ†å—é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
        
    except Exception as e:
        print(f"âš ï¸ [WARNING] è·å–æ–‡æ¡£åˆ†å—é…ç½®å¤±è´¥: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def _log_performance_stats(operation_name, start_time, end_time, item_count, additional_info=None):
    """è®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    if not CHUNK_PROCESSING_CONFIG.get('enable_performance_stats', True):
        return
        
    duration = end_time - start_time
    throughput = item_count / duration if duration > 0 else 0
    
    stats_msg = f"[æ€§èƒ½ç»Ÿè®¡] {operation_name}: "
    stats_msg += f"è€—æ—¶ {duration:.2f}s, "
    stats_msg += f"å¤„ç† {item_count} é¡¹, "
    stats_msg += f"ååé‡ {throughput:.2f} é¡¹/ç§’"
    
    if additional_info:
        stats_msg += f", {additional_info}"
    
    print(stats_msg)
    
    # å¦‚æœè€—æ—¶è¿‡é•¿ï¼Œè®°å½•è­¦å‘Š
    if duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
        print(f"[æ€§èƒ½è­¦å‘Š] {operation_name} å¤„ç†æ—¶é—´è¿‡é•¿: {duration:.2f}s")

def add_chunks_to_doc(doc, chunks, update_progress, config=None):
    start_time = time.time()
    
    # åˆå¹¶é…ç½®å‚æ•°
    effective_config = CHUNK_PROCESSING_CONFIG.copy()
    if config:
        effective_config.update(config)
    
    print(f"æ€»å…±æ¥æ”¶åˆ° {len(chunks)} ä¸ª chunks å‡†å¤‡æ·»åŠ ã€‚")
    
    # é…ç½®å¹¶å‘å‚æ•°
    max_workers = min(effective_config['max_concurrent_workers'], len(chunks))
    chunk_results = [None] * len(chunks)  # ä¿æŒé¡ºåºçš„ç»“æœæ•°ç»„
    failed_chunks = []
    lock = threading.Lock()
    completed_count = 0
    
    def add_single_chunk(index, chunk):
        """æ·»åŠ å•ä¸ªchunkçš„å‡½æ•°"""
        nonlocal completed_count
        chunk_start_time = time.time()
        try:
            chunk_preview = chunk.strip()[:50].replace('\n', ' ')
            print(f"æ­£åœ¨å¤„ç† Chunk {index}: \"{chunk_preview}...\"")
            
            if chunk and chunk.strip():
                doc.add_chunk(content=chunk)
                
                # æ›´æ–°è¿›åº¦
                with lock:
                    completed_count += 1
                    progress = 0.8 + (completed_count / len(chunks)) * 0.15  # 0.8-0.95èŒƒå›´
                    update_progress(progress, f"æ·»åŠ chunksè¿›åº¦: {completed_count}/{len(chunks)}")
                
                chunk_duration = time.time() - chunk_start_time
                if chunk_duration > 5:  # å•ä¸ªchunkå¤„ç†è¶…è¿‡5ç§’
                    print(f"[æ€§èƒ½è­¦å‘Š] Chunk {index} å¤„ç†æ—¶é—´è¾ƒé•¿: {chunk_duration:.2f}s")
                
                return index, True, None
            else:
                with lock:
                    completed_count += 1
                    progress = 0.8 + (completed_count / len(chunks)) * 0.15
                    update_progress(progress, f"æ·»åŠ chunksè¿›åº¦: {completed_count}/{len(chunks)}")
                return index, False, "chunkå†…å®¹ä¸ºç©º"
        except Exception as e:
            print(f"æ·»åŠ  chunk {index} å¤±è´¥: {e}")
            with lock:
                completed_count += 1
                progress = 0.8 + (completed_count / len(chunks)) * 0.15
                update_progress(progress, f"æ·»åŠ chunksè¿›åº¦: {completed_count}/{len(chunks)}")
            return index, False, str(e)
    
    # åˆå§‹è¿›åº¦æ›´æ–°
    update_progress(0.8, "å¼€å§‹æ·»åŠ chunksåˆ°æ–‡æ¡£...")
    
    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†
    use_concurrent = (
        effective_config['enable_concurrent_chunk_add'] and 
        len(chunks) > 1 and 
        max_workers > 1
    )
    
    processing_start_time = time.time()
    
    try:
        if use_concurrent:
            print(f"ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘æ·»åŠ chunks...")
            
            try:
                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œä¿æŒç´¢å¼•æ˜ å°„
                    future_to_index = {
                        executor.submit(add_single_chunk, i, chunk): i 
                        for i, chunk in enumerate(chunks)
                    }
                    
                    # æ”¶é›†ç»“æœï¼Œä¿æŒåŸå§‹é¡ºåº
                    try:
                        for future in concurrent.futures.as_completed(future_to_index, timeout=effective_config['chunk_add_timeout']):
                            index, success, error = future.result()
                            
                            with lock:
                                chunk_results[index] = success
                                if not success:
                                    failed_chunks.append((index, error))
                    except concurrent.futures.TimeoutError:
                        print(f"[å¼‚å¸¸å¤„ç†] å¹¶å‘å¤„ç†è¶…æ—¶ ({effective_config['chunk_add_timeout']}s)ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                        # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                        for future in future_to_index:
                            if not future.done():
                                future.cancel()
                        
                        # æ”¶é›†å·²å®Œæˆçš„ç»“æœ
                        for future in future_to_index:
                            if future.done() and not future.cancelled():
                                try:
                                    index, success, error = future.result()
                                    with lock:
                                        chunk_results[index] = success
                                        if not success:
                                            failed_chunks.append((index, error))
                                except Exception as e:
                                    print(f"[å¼‚å¸¸å¤„ç†] è·å–è¶…æ—¶ä»»åŠ¡ç»“æœå¤±è´¥: {e}")
                        
                        # å°†æœªå®Œæˆçš„chunksæ ‡è®°ä¸ºå¤±è´¥
                        for future, index in future_to_index.items():
                            if future.cancelled() or not future.done():
                                chunk_results[index] = False
                                failed_chunks.append((index, "ä»»åŠ¡è¶…æ—¶è¢«å–æ¶ˆ"))
                        
                        print(f"[å¼‚å¸¸å¤„ç†] è¶…æ—¶å¤„ç†å®Œæˆï¼Œå·²å¤„ç†çš„chunks: {completed_count}/{len(chunks)}")
                        
            except Exception as concurrent_e:
                print(f"[å¼‚å¸¸å¤„ç†] å¹¶å‘æ‰§è¡Œå‡ºç°å¼‚å¸¸: {concurrent_e}")
                # å›é€€åˆ°å•çº¿ç¨‹æ¨¡å¼
                print("[å¼‚å¸¸å¤„ç†] å›é€€åˆ°å•çº¿ç¨‹æ¨¡å¼...")
                use_concurrent = False  # æ ‡è®°ä¸ºéå¹¶å‘æ¨¡å¼ï¼Œç”¨äºåç»­ç»Ÿè®¡
                
        if not use_concurrent:
            # å•çº¿ç¨‹å¤„ç†
            print("ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼æ·»åŠ chunks...")
            for i, chunk in enumerate(chunks):
                try:
                    index, success, error = add_single_chunk(i, chunk)
                    chunk_results[index] = success
                    if not success:
                        failed_chunks.append((index, error))
                except Exception as e:
                    print(f"[å¼‚å¸¸å¤„ç†] å•çº¿ç¨‹å¤„ç†Chunk {i}å¤±è´¥: {e}")
                    chunk_results[i] = False
                    failed_chunks.append((i, f"å•çº¿ç¨‹å¤„ç†å¼‚å¸¸: {str(e)}"))
        
    except Exception as overall_e:
        print(f"[å¼‚å¸¸å¤„ç†] æ•´ä½“å¤„ç†å‡ºç°å¼‚å¸¸: {overall_e}")
        # ç¡®ä¿æœ‰åŸºç¡€çš„ç»“æœæ•°ç»„
        if not chunk_results or all(x is None for x in chunk_results):
            chunk_results = [False] * len(chunks)
            failed_chunks = [(i, f"æ•´ä½“å¤„ç†å¼‚å¸¸: {str(overall_e)}") for i in range(len(chunks))]
    
    finally:
        # ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°0.95ï¼Œæ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸
        processing_end_time = time.time()
        
        # ç»Ÿè®¡ç»“æœ
        successful_count = sum(1 for result in chunk_results if result)
        update_progress(0.95, f"Chunksæ·»åŠ å®Œæˆ: æˆåŠŸ {successful_count}/{len(chunks)}")
        print(f"Chunksæ·»åŠ å®Œæˆ: æˆåŠŸ {successful_count}/{len(chunks)}")
        
        if failed_chunks:
            print(f"å¤±è´¥çš„chunksç´¢å¼•: {[idx for idx, _ in failed_chunks]}")
            for idx, error in failed_chunks[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"  Chunk {idx} å¤±è´¥åŸå› : {error}")
            if len(failed_chunks) > 5:
                print(f"  ... è¿˜æœ‰ {len(failed_chunks) - 5} ä¸ªå¤±è´¥çš„chunks")
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        end_time = time.time()
        mode = "å¹¶å‘æ¨¡å¼" if use_concurrent else "å•çº¿ç¨‹æ¨¡å¼"
        additional_info = f"{mode}, å·¥ä½œçº¿ç¨‹æ•°: {max_workers if use_concurrent else 1}, æˆåŠŸç‡: {successful_count/len(chunks)*100:.1f}%"
        _log_performance_stats("æ·»åŠ Chunks", processing_start_time, processing_end_time, len(chunks), additional_info)
    
    return successful_count

def _update_chunks_position(doc, md_file_path, chunk_content_to_index, config=None, update_progress=None):
    start_time = time.time()
    
    # åˆå¹¶é…ç½®å‚æ•°
    effective_config = CHUNK_PROCESSING_CONFIG.copy()
    if config:
        effective_config.update(config)
        
    es_client = get_es_client()
    print(f"æ–‡æ¡£: id: {doc.id})")
    chunk_count = 0
    tenant_id = doc.created_by
    index_name = f"ragflow_{tenant_id}"
    
    # æ”¶é›†æ‰€æœ‰æ‰¹é‡æ›´æ–°æ“ä½œ
    bulk_operations = []
    batch_size = effective_config['es_bulk_batch_size']
    processed_count = 0
    batch_count = 0
    
    try:
        # è·å–æ€»chunkæ•°é‡ç”¨äºè¿›åº¦è®¡ç®—
        all_chunks = list(doc.list_chunks(keywords=None, page=1, page_size=10000))
        total_chunks = len(all_chunks)
        print(f"å‡†å¤‡æ›´æ–° {total_chunks} ä¸ªchunksçš„ä½ç½®ä¿¡æ¯...")
        
        if update_progress:
            update_progress(0.96, "å¼€å§‹æ›´æ–°chunkä½ç½®ä¿¡æ¯...")
        
        position_fetch_time = 0
        
        for chunk in all_chunks:
            try:
                original_index = chunk_content_to_index.get(chunk.content)
                if original_index is None:
                    print(f"è­¦å‘Š: æ— æ³•ä¸ºå— id={chunk.id} çš„å†…å®¹æ‰¾åˆ°åŸå§‹ç´¢å¼•ï¼Œå°†è·³è¿‡æ­¤å—ã€‚")
                    processed_count += 1
                    continue
                
                # æ„å»ºæ›´æ–°æ“ä½œ - ä½¿ç”¨æ­£ç¡®çš„ES bulkæ ¼å¼
                doc_update = {
                    "top_int": original_index
                }
                
                # å°è¯•è·å–ä½ç½®ä¿¡æ¯ï¼Œå¦‚æœæˆåŠŸåˆ™æ·»åŠ åˆ°æ›´æ–°ä¸­
                position_start = time.time()
                try:
                    position_int_temp = get_bbox_for_chunk(md_file_path, chunk.content)
                    if position_int_temp is not None:
                        doc_fields = {}
                        _add_positions(doc_fields, position_int_temp)
                        doc_update["position_int"] = doc_fields.get("position_int")
                except Exception as e:
                    print(f"è·å–chunkä½ç½®å¼‚å¸¸: {e}")
                position_fetch_time += time.time() - position_start
                
                # ES bulkæ ¼å¼ï¼šactionè¡Œ + documentè¡Œ
                bulk_operations.extend([
                    {"update": {"_index": index_name, "_id": chunk.id}},
                    {"doc": doc_update}
                ])
                chunk_count += 1
                processed_count += 1
                
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
                if len(bulk_operations) >= batch_size:
                    try:
                        batch_start = time.time()
                        _execute_bulk_update(es_client, bulk_operations, effective_config)
                        batch_duration = time.time() - batch_start
                        batch_count += 1
                        
                        print(f"[æ‰¹æ¬¡ {batch_count}] æ›´æ–° {len(bulk_operations)} ä¸ªchunks, è€—æ—¶ {batch_duration:.2f}s")
                        bulk_operations = []
                        
                        # æ›´æ–°è¿›åº¦
                        if update_progress:
                            progress = 0.96 + (processed_count / total_chunks) * 0.03  # 0.96-0.99èŒƒå›´
                            update_progress(progress, f"æ›´æ–°ä½ç½®è¿›åº¦: {processed_count}/{total_chunks}")
                    except Exception as batch_e:
                        print(f"[å¼‚å¸¸å¤„ç†] æ‰¹æ¬¡æ›´æ–°å¤±è´¥: {batch_e}")
                        # æ‰¹æ¬¡å¤±è´¥æ—¶ï¼Œæ¸…ç©ºå½“å‰æ‰¹æ¬¡ç»§ç»­å¤„ç†
                        bulk_operations = []
                        
            except Exception as chunk_e:
                print(f"[å¼‚å¸¸å¤„ç†] å¤„ç†chunk {chunk.id} å¤±è´¥: {chunk_e}")
                processed_count += 1
                continue
        
        # å¤„ç†å‰©ä½™çš„æ“ä½œ
        if bulk_operations:
            try:
                batch_start = time.time()
                _execute_bulk_update(es_client, bulk_operations, effective_config)
                batch_duration = time.time() - batch_start
                batch_count += 1
                print(f"[æœ€ç»ˆæ‰¹æ¬¡ {batch_count}] æ›´æ–° {len(bulk_operations)} ä¸ªchunks, è€—æ—¶ {batch_duration:.2f}s")
            except Exception as final_batch_e:
                print(f"[å¼‚å¸¸å¤„ç†] æœ€ç»ˆæ‰¹æ¬¡æ›´æ–°å¤±è´¥: {final_batch_e}")
        
    except Exception as overall_e:
        print(f"[å¼‚å¸¸å¤„ç†] ä½ç½®æ›´æ–°æ•´ä½“å¤„ç†å‡ºç°å¼‚å¸¸: {overall_e}")
        
    finally:
        # ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°0.99ï¼Œæ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸
        if update_progress:
            update_progress(0.99, f"ä½ç½®æ›´æ–°å®Œæˆ: {chunk_count} ä¸ªchunks")
        
        end_time = time.time()
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        additional_info = f"æ‰¹æ¬¡æ•°: {batch_count}, æ‰¹æ¬¡å¤§å°: {batch_size}, ä½ç½®è·å–è€—æ—¶: {position_fetch_time:.2f}s"
        _log_performance_stats("æ›´æ–°Chunkä½ç½®", start_time, end_time, chunk_count, additional_info)
        
        print(f"ä½ç½®æ›´æ–°å®Œæˆ: {chunk_count} ä¸ªchunks")
    
    return chunk_count

def _execute_bulk_update(es_client, bulk_operations, config):
    """æ‰§è¡ŒESæ‰¹é‡æ›´æ–°çš„è¾…åŠ©å‡½æ•°"""
    if not bulk_operations:
        return
        
    operation_start = time.time()
    
    try:
        print(f"å¼€å§‹æ‰¹é‡æ›´æ–° {len(bulk_operations)} ä¸ªchunks...")
        response = es_client.bulk(
            body=bulk_operations, 
            refresh=True,
            timeout=f"{config['es_bulk_timeout']}s"
        )
        
        operation_duration = time.time() - operation_start
        
        # æ£€æŸ¥æ‰¹é‡æ“ä½œç»“æœ
        if response.get('errors'):
            failed_count = 0
            for item in response.get('items', []):
                if 'update' in item and item['update'].get('status') >= 400:
                    print(f"ESæ‰¹é‡æ›´æ–°å¤±è´¥ - ID: {item['update'].get('_id')}, Error: {item['update'].get('error')}")
                    failed_count += 1
            
            if failed_count > 0:
                print(f"æ‰¹é‡æ›´æ–°å®Œæˆï¼Œä½†æœ‰ {failed_count} ä¸ªæ“ä½œå¤±è´¥, è€—æ—¶ {operation_duration:.2f}s")
            else:
                print(f"æ‰¹é‡æ›´æ–°æˆåŠŸå®Œæˆ {len(bulk_operations) // 2} ä¸ªchunks, è€—æ—¶ {operation_duration:.2f}s")
        else:
            throughput = (len(bulk_operations) // 2) / operation_duration if operation_duration > 0 else 0
            print(f"æ‰¹é‡æ›´æ–°æˆåŠŸå®Œæˆ {len(bulk_operations) // 2} ä¸ªchunks, è€—æ—¶ {operation_duration:.2f}s, ååé‡ {throughput:.1f} chunks/ç§’")
            
    except Exception as es_e:
        operation_duration = time.time() - operation_start
        print(f"ESæ‰¹é‡æ›´æ–°å¼‚å¸¸: {es_e}, è€—æ—¶ {operation_duration:.2f}s")
        # å¦‚æœæ‰¹é‡æ›´æ–°å¤±è´¥ï¼Œå›é€€åˆ°å•ä¸ªæ›´æ–°æ¨¡å¼
        print("å›é€€åˆ°å•ä¸ªæ›´æ–°æ¨¡å¼...")
        fallback_start = time.time()
        success_count = 0
        
        # å¤„ç†æ–°çš„ä¸¤è¡Œæ ¼å¼ï¼šæ¯ä¸¤ä¸ªå…ƒç´ æ„æˆä¸€ä¸ªæ“ä½œ
        for i in range(0, len(bulk_operations), 2):
            if i + 1 < len(bulk_operations):
                try:
                    action = bulk_operations[i]
                    doc_data = bulk_operations[i + 1]
                    
                    if "update" in action:
                        es_client.update(
                            index=action["update"]["_index"], 
                            id=action["update"]["_id"], 
                            body=doc_data, 
                            refresh=True
                        )
                        success_count += 1
                except Exception as single_e:
                    print(f"å•ä¸ªæ›´æ–°ä¹Ÿå¤±è´¥ - ID: {action.get('update', {}).get('_id', 'unknown')}, Error: {single_e}")
        
        fallback_duration = time.time() - fallback_start
        expected_operations = len(bulk_operations) // 2
        print(f"å›é€€æ¨¡å¼å®Œæˆ: æˆåŠŸ {success_count}/{expected_operations} ä¸ªchunks, è€—æ—¶ {fallback_duration:.2f}s")

def _cleanup_temp_files(md_file_path):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if not should_cleanup_temp_files():
        print(f"[INFO] é…ç½®ä¸ºä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œè·¯å¾„: {os.path.dirname(os.path.abspath(md_file_path))}")
        return
    
    try:
        temp_dir = os.path.dirname(os.path.abspath(md_file_path))
        shutil.rmtree(temp_dir)
        print(f"[INFO] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç›®å½•: {temp_dir}")
    except Exception as e:
        print(f"[WARNING] æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¼‚å¸¸: {e}")

def create_ragflow_resources(doc_id, kb_id, md_file_path, image_dir, update_progress):
    """
    ä½¿ç”¨å¢å¼ºæ–‡æœ¬åˆ›å»ºRAGFlowçŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹
    """
    try:
        doc = get_ragflow_doc(doc_id, kb_id)

        _upload_images(kb_id, image_dir, update_progress)

        # è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®
        chunking_config = _get_document_chunking_config(doc_id)
        
        enhanced_text = update_markdown_image_urls(md_file_path, kb_id)
        
        # ä¼ é€’åˆ†å—é…ç½®ç»™åˆ†å—å‡½æ•°
        if chunking_config:
            chunks = split_markdown_to_chunks_configured(
                enhanced_text, 
                chunk_token_num=chunking_config.get('chunk_token_num', 256),
                min_chunk_tokens=chunking_config.get('min_chunk_tokens', 10),
                chunking_config=chunking_config
            )
        else:
            chunks = split_markdown_to_chunks_configured(enhanced_text, chunk_token_num=256)
        
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks)}

        add_chunks_to_doc(doc, chunks, update_progress)
        chunk_count = _update_chunks_position(doc, md_file_path, chunk_content_to_index, update_progress=update_progress)
        # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        _cleanup_temp_files(md_file_path)

        # ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°100%
        update_progress(1.0, f"å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {chunk_count} ä¸ªchunks")
        print(f"âœ… æ‰€æœ‰å¤„ç†æ­¥éª¤å®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªchunks")

        return chunk_count

    except Exception as e:
        print(f"create_ragflow_resources å¤„ç†å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # å³ä½¿å‘ç”Ÿå¼‚å¸¸ï¼Œä¹Ÿè¦ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°100%ï¼Œé¿å…å‰ç«¯ç•Œé¢å¡ä½
        try:
            update_progress(1.0, f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œä½†è¿›åº¦å·²æ›´æ–°å®Œæˆ")
        except Exception as progress_e:
            print(f"[å¼‚å¸¸å¤„ç†] æ›´æ–°è¿›åº¦æ—¶ä¹Ÿå‘ç”Ÿå¼‚å¸¸: {progress_e}")
        
        raise

def _add_positions(d, poss):
    try:
        if not poss:
            return
        page_num_int = []
        position_int = []
        top_int = []
        for pn, left, right, top, bottom in poss:
            page_num_int.append(int(pn + 1))
            top_int.append(int(top))
            position_int.append((int(pn + 1), int(left), int(right), int(top), int(bottom)))
        d["page_num_int"] = page_num_int
        d["position_int"] = position_int
        d["top_int"] = top_int
    except Exception as e:
        print(f"add_positionså¼‚å¸¸: {e}")