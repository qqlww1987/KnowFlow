from ragflow_sdk import RAGFlow
import os
import time
import shutil
import json
from dotenv import load_dotenv
from .minio_server import upload_directory_to_minio
from .mineru_test import update_markdown_image_urls
from .utils import split_markdown_to_chunks_configured, get_bbox_for_chunk, update_document_progress, should_cleanup_temp_files
from database import get_db_connection
from datetime import datetime

# æ€§èƒ½ä¼˜åŒ–é…ç½®å‚æ•°
CHUNK_PROCESSING_CONFIG = {
    'max_concurrent_workers': 8,           # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
    'es_bulk_batch_size': 100,           # ESæ‰¹é‡æ“ä½œæ‰¹æ¬¡å¤§å°
    'enable_concurrent_chunk_add': True,   # æ˜¯å¦å¯ç”¨å¹¶å‘æ·»åŠ chunks
    'chunk_add_timeout': 30,              # å•ä¸ªchunkæ·»åŠ è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'es_bulk_timeout': 60,                # ESæ‰¹é‡æ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'enable_performance_stats': True,     # æ˜¯å¦å¯ç”¨æ€§èƒ½ç»Ÿè®¡
}

def _validate_environment():
    """éªŒè¯ç¯å¢ƒå˜é‡é…ç½®"""
    load_dotenv()
    api_key = os.getenv('RAGFLOW_API_KEY')
    base_url = os.getenv('RAGFLOW_BASE_URL')
    if not api_key:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_API_KEYæˆ–ä½¿ç”¨--api_keyå‚æ•°æŒ‡å®šã€‚")
    if not base_url:
        raise ValueError("é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®RAGFLOW_BASE_URLæˆ–ä½¿ç”¨--server_ipå‚æ•°æŒ‡å®šã€‚")
    return api_key, base_url

def _upload_images(kb_id, image_dir, update_progress):
    update_progress(0.7, "ä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    print(f"ç¬¬4æ­¥ï¼šä¸Šä¼ å›¾ç‰‡åˆ°MinIO...")
    upload_directory_to_minio(kb_id, image_dir)

def get_ragflow_doc(doc_id, kb_id):
    """è·å–RAGFlowæ–‡æ¡£å¯¹è±¡å’Œdatasetå¯¹è±¡"""
    api_key, base_url = _validate_environment()
    rag_object = RAGFlow(api_key=api_key, base_url=base_url)
    datasets = rag_object.list_datasets(id=kb_id)
    if not datasets:
        raise Exception(f"æœªæ‰¾åˆ°çŸ¥è¯†åº“ {kb_id}")
    dataset = datasets[0]
    docs = dataset.list_documents(id=doc_id)
    if not docs:
        raise Exception(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_id}")
    return docs[0], dataset  # è¿”å›docå’Œdatasetå…ƒç»„

def _get_document_chunking_config(doc_id):
    """ä»æ•°æ®åº“è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT parser_config FROM document WHERE id = %s", (doc_id,))
        result = cursor.fetchone()
        
        if result and result[0]:
            parser_config = json.loads(result[0])
            chunking_config = parser_config.get('chunking_config')
            if chunking_config:
                print(f"ğŸ”§ [DEBUG] ä»æ•°æ®åº“è·å–åˆ°åˆ†å—é…ç½®: {chunking_config}")
                return chunking_config
        
        print(f"ğŸ“„ [DEBUG] æ–‡æ¡£ {doc_id} æ²¡æœ‰è‡ªå®šä¹‰åˆ†å—é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None
        
    except Exception as e:
        print(f"âš ï¸ [WARNING] è·å–æ–‡æ¡£åˆ†å—é…ç½®å¤±è´¥: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def _log_performance_stats(operation_name, start_time, end_time, item_count, additional_info=None):
    """è®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    if not CHUNK_PROCESSING_CONFIG.get('enable_performance_stats', True):
        return
        
    duration = end_time - start_time
    throughput = item_count / duration if duration > 0 else 0
    
    stats_msg = f"[æ€§èƒ½ç»Ÿè®¡] {operation_name}: "
    stats_msg += f"è€—æ—¶ {duration:.2f}s, "
    stats_msg += f"å¤„ç† {item_count} é¡¹, "
    stats_msg += f"ååé‡ {throughput:.2f} é¡¹/ç§’"
    
    if additional_info:
        stats_msg += f", {additional_info}"
    
    print(stats_msg)
    
    # å¦‚æœè€—æ—¶è¿‡é•¿ï¼Œè®°å½•è­¦å‘Š
    if duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
        print(f"[æ€§èƒ½è­¦å‘Š] {operation_name} å¤„ç†æ—¶é—´è¿‡é•¿: {duration:.2f}s")

def add_chunks_with_positions(doc, chunks, md_file_path, chunk_content_to_index, update_progress, config=None):
    """
    åˆå¹¶ç‰ˆ add_chunks_to_doc + _update_chunks_position
    ç›´æ¥è°ƒç”¨ batch_add_chunk æ¥å£ï¼Œä¸€æ­¥å®Œæˆchunkæ·»åŠ å’Œä½ç½®ä¿¡æ¯è®¾ç½®
    """
    start_time = time.time()
    
    # åˆå¹¶é…ç½®å‚æ•°
    effective_config = CHUNK_PROCESSING_CONFIG.copy()
    if config:
        effective_config.update(config)
    
    print(f"ğŸš€ åˆå¹¶æ‰¹é‡æ·»åŠ : æ€»å…±æ¥æ”¶åˆ° {len(chunks)} ä¸ª chunks å‡†å¤‡æ‰¹é‡æ·»åŠ ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰")
    
    if not chunks:
        print("âš ï¸ æ²¡æœ‰chunkséœ€è¦æ·»åŠ ")
        update_progress(0.8, "æ²¡æœ‰chunkséœ€è¦æ·»åŠ ")
        return 0
    
    # åˆå§‹è¿›åº¦æ›´æ–°
    update_progress(0.8, "å¼€å§‹æ‰¹é‡æ·»åŠ chunksåˆ°æ–‡æ¡£ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰...")
    
    try:
        # å‡†å¤‡æ‰¹é‡æ•°æ®ï¼ŒåŒ…å«ä½ç½®ä¿¡æ¯
        batch_chunks = []
        for i, chunk in enumerate(chunks):
            if chunk and chunk.strip():
                chunk_data = {
                    "content": chunk.strip(),
                    "important_keywords": [],  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…³é”®è¯æå–
                    "questions": []  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ é—®é¢˜ç”Ÿæˆ
                }
                
                # è·å–ä½ç½®ä¿¡æ¯
                try:
                    position_int_temp = get_bbox_for_chunk(md_file_path, chunk.strip())
                    if position_int_temp is not None:
                        # æœ‰å®Œæ•´ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨positionså‚æ•°
                        chunk_data["positions"] = position_int_temp
                        print(f"ğŸ”§ chunk {i}: è·å–åˆ°å®Œæ•´ä½ç½®ä¿¡æ¯: {len(position_int_temp)} ä¸ªä½ç½®")
                    else:
                        # æ²¡æœ‰å®Œæ•´ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨top_intå‚æ•°
                        original_index = chunk_content_to_index.get(chunk.strip())
                        if original_index is not None:
                            chunk_data["top_int"] = original_index
                            print(f"ğŸ”§ chunk {i}: ä½¿ç”¨top_int: {original_index}")
                        else:
                            print(f"âš ï¸ chunk {i}: æ— æ³•è·å–ä½ç½®ä¿¡æ¯")
                except Exception as pos_e:
                    print(f"âš ï¸ chunk {i}: è·å–ä½ç½®ä¿¡æ¯å¤±è´¥: {pos_e}")
                    # å³ä½¿ä½ç½®ä¿¡æ¯è·å–å¤±è´¥ï¼Œä¹Ÿç»§ç»­æ·»åŠ chunk
                
                batch_chunks.append(chunk_data)
        
        if not batch_chunks:
            print("âš ï¸ è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„chunks")
            update_progress(0.95, "æ²¡æœ‰æœ‰æ•ˆçš„chunks")
            return 0
        
        print(f"ğŸ“¦ å‡†å¤‡æ‰¹é‡æ·»åŠ  {len(batch_chunks)} ä¸ªæœ‰æ•ˆchunksï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰")
        
        # ç»Ÿè®¡ä½ç½®ä¿¡æ¯ç±»å‹
        chunks_with_positions = [c for c in batch_chunks if "positions" in c]
        chunks_with_top_int = [c for c in batch_chunks if "top_int" in c]
        chunks_without_position = len(batch_chunks) - len(chunks_with_positions) - len(chunks_with_top_int)
        
        print(f"ğŸ“ ä½ç½®ä¿¡æ¯ç»Ÿè®¡:")
        print(f"   - å®Œæ•´ä½ç½®ä¿¡æ¯: {len(chunks_with_positions)} chunks")
        print(f"   - å•ç‹¬top_int: {len(chunks_with_top_int)} chunks")
        print(f"   - æ— ä½ç½®ä¿¡æ¯: {chunks_without_position} chunks")
        
        # é…ç½®æ‰¹é‡å¤§å° - æ ¹æ®chunkæ•°é‡åŠ¨æ€è°ƒæ•´
        if len(batch_chunks) <= 10:
            batch_size = 5
        elif len(batch_chunks) <= 50:
            batch_size = 10
        else:
            batch_size = 20
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
        total_added = 0
        total_failed = 0
        batch_count = (len(batch_chunks) + batch_size - 1) // batch_size
        
        for batch_idx in range(0, len(batch_chunks), batch_size):
            batch_end = min(batch_idx + batch_size, len(batch_chunks))
            current_batch = batch_chunks[batch_idx:batch_end]
            
            current_batch_num = batch_idx // batch_size + 1
            print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {current_batch_num}/{batch_count} ({len(current_batch)} chunks)")
            
            try:
                # ç›´æ¥è°ƒç”¨æ‰¹é‡æ¥å£
                response = doc.rag.post(
                    f'/datasets/{doc.dataset_id}/documents/{doc.id}/chunks/batch',
                    {
                        "chunks": current_batch,
                        "batch_size": min(batch_size, len(current_batch))
                    }
                )
                
                result = response.json()
                
                if result.get("code") == 0:
                    # æ‰¹é‡æ·»åŠ æˆåŠŸ
                    data = result.get("data", {})
                    added = data.get("total_added", 0)
                    failed = data.get("total_failed", 0)
                    
                    total_added += added
                    total_failed += failed
                    
                    # æ›´æ–°è¿›åº¦
                    progress = 0.8 + (batch_end / len(batch_chunks)) * 0.15  # 0.8-0.95èŒƒå›´
                    update_progress(progress, f"æ‰¹é‡æ·»åŠ è¿›åº¦: {batch_end}/{len(batch_chunks)} chunks")
                    
                    print(f"âœ… æ‰¹æ¬¡ {current_batch_num} æˆåŠŸ: +{added} chunks (å¤±è´¥: {failed})")
                    
                    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
                    stats = data.get("processing_stats", {})
                    if stats:
                        print(f"   ğŸ“Š åˆ†ç‰‡å¤„ç†: {stats.get('batches_processed', 0)} ä¸ªåˆ†ç‰‡")
                        print(f"   ğŸ’° åµŒå…¥æˆæœ¬: {stats.get('embedding_cost', 0)}")
                    
                    # æ£€æŸ¥è¿”å›çš„chunksæ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
                    returned_chunks = data.get("chunks", [])
                    if returned_chunks:
                        chunks_with_pos = [c for c in returned_chunks if c.get('positions') or c.get('top_positions')]
                        print(f"   ğŸ“ ä½ç½®ä¿¡æ¯: {len(chunks_with_pos)}/{len(returned_chunks)} chunksåŒ…å«ä½ç½®")
                
                else:
                    # æ‰¹é‡æ·»åŠ å¤±è´¥
                    error_msg = result.get("message", "Unknown error")
                    print(f"âŒ æ‰¹æ¬¡ {current_batch_num} å¤±è´¥: {error_msg}")
                    total_failed += len(current_batch)
                    
                    # æ›´æ–°è¿›åº¦
                    progress = 0.8 + (batch_end / len(batch_chunks)) * 0.15
                    update_progress(progress, f"æ‰¹é‡æ·»åŠ è¿›åº¦: {batch_end}/{len(batch_chunks)} chunks (éƒ¨åˆ†å¤±è´¥)")
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {current_batch_num} ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
                total_failed += len(current_batch)
                
                # æ›´æ–°è¿›åº¦
                progress = 0.8 + (batch_end / len(batch_chunks)) * 0.15
                update_progress(progress, f"æ‰¹é‡æ·»åŠ è¿›åº¦: {batch_end}/{len(batch_chunks)} chunks (ç½‘ç»œå¼‚å¸¸)")
        
        # æœ€ç»ˆç»Ÿè®¡
        success_rate = (total_added / len(batch_chunks) * 100) if len(batch_chunks) > 0 else 0
        
        print(f"ğŸ“Š åˆå¹¶æ‰¹é‡æ·»åŠ å®Œæˆ:")
        print(f"   âœ… æˆåŠŸ: {total_added}/{len(batch_chunks)} chunks")
        print(f"   âŒ å¤±è´¥: {total_failed} chunks") 
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   ğŸ“ ä½ç½®ä¿¡æ¯: {len(chunks_with_positions)} å®Œæ•´ä½ç½®, {len(chunks_with_top_int)} top_int")
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        if total_failed == 0:
            update_progress(0.95, f"æ‰¹é‡æ·»åŠ å®Œæˆ: æˆåŠŸ {total_added}/{len(batch_chunks)} chunksï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰")
        else:
            update_progress(0.95, f"æ‰¹é‡æ·»åŠ å®Œæˆ: æˆåŠŸ {total_added}, å¤±è´¥ {total_failed} chunks")
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        end_time = time.time()
        processing_time = end_time - start_time
        additional_info = f"åˆå¹¶æ¨¡å¼, æ‰¹æ¬¡æ•°: {batch_count}, æˆåŠŸç‡: {success_rate:.1f}%, ä½ç½®ä¿¡æ¯: {len(chunks_with_positions)}+{len(chunks_with_top_int)}"
        _log_performance_stats("åˆå¹¶æ‰¹é‡æ·»åŠ Chunks", start_time, end_time, len(batch_chunks), additional_info)
        
        return total_added
        
    except Exception as e:
        print(f"âŒ åˆå¹¶æ‰¹é‡æ·»åŠ è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        update_progress(0.95, f"æ‰¹é‡æ·»åŠ å¼‚å¸¸: {str(e)}")
        
        # è®°å½•å¼‚å¸¸ç»Ÿè®¡
        end_time = time.time()
        _log_performance_stats("åˆå¹¶æ‰¹é‡æ·»åŠ Chunks(å¼‚å¸¸)", start_time, end_time, len(chunks), f"å¼‚å¸¸: {str(e)}")
        
        return 0

def _cleanup_temp_files(md_file_path):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    if not should_cleanup_temp_files():
        print(f"[INFO] é…ç½®ä¸ºä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œè·¯å¾„: {os.path.dirname(os.path.abspath(md_file_path))}")
        return
    
    try:
        temp_dir = os.path.dirname(os.path.abspath(md_file_path))
        shutil.rmtree(temp_dir)
        print(f"[INFO] å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶ç›®å½•: {temp_dir}")
    except Exception as e:
        print(f"[WARNING] æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¼‚å¸¸: {e}")

def create_ragflow_resources(doc_id, kb_id, md_file_path, image_dir, update_progress):
    """
    ä½¿ç”¨å¢å¼ºæ–‡æœ¬åˆ›å»ºRAGFlowçŸ¥è¯†åº“å’ŒèŠå¤©åŠ©æ‰‹
    """
    try:
        doc, dataset = get_ragflow_doc(doc_id, kb_id)

        _upload_images(kb_id, image_dir, update_progress)

        # è·å–æ–‡æ¡£çš„åˆ†å—é…ç½®
        chunking_config = _get_document_chunking_config(doc_id)
        
        enhanced_text = update_markdown_image_urls(md_file_path, kb_id)
        
        # ä¼ é€’åˆ†å—é…ç½®ç»™åˆ†å—å‡½æ•°
        if chunking_config:
            chunks = split_markdown_to_chunks_configured(
                enhanced_text, 
                chunk_token_num=chunking_config.get('chunk_token_num', 256),
                min_chunk_tokens=chunking_config.get('min_chunk_tokens', 10),
                chunking_config=chunking_config
            )
        else:
            chunks = split_markdown_to_chunks_configured(enhanced_text, chunk_token_num=256)
        
        chunk_content_to_index = {chunk: i for i, chunk in enumerate(chunks)}

        chunk_count = add_chunks_with_positions(doc, chunks, md_file_path, chunk_content_to_index, update_progress)
        # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        _cleanup_temp_files(md_file_path)

        # ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°100%
        update_progress(1.0, f"å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {chunk_count} ä¸ªchunks")
        print(f"âœ… æ‰€æœ‰å¤„ç†æ­¥éª¤å®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªchunks")

        return chunk_count

    except Exception as e:
        print(f"create_ragflow_resources å¤„ç†å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # å³ä½¿å‘ç”Ÿå¼‚å¸¸ï¼Œä¹Ÿè¦ç¡®ä¿è¿›åº¦æ›´æ–°åˆ°100%ï¼Œé¿å…å‰ç«¯ç•Œé¢å¡ä½
        try:
            update_progress(1.0, f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œä½†è¿›åº¦å·²æ›´æ–°å®Œæˆ")
        except Exception as progress_e:
            print(f"[å¼‚å¸¸å¤„ç†] æ›´æ–°è¿›åº¦æ—¶ä¹Ÿå‘ç”Ÿå¼‚å¸¸: {progress_e}")
        
        raise
